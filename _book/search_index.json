[["index.html", "Statistics: A Critical Look Preface", " Statistics: A Critical Look Preface This book grew out of my class notes in Stat-2 and my observations of other instructors around the world. I claim no originality in my thoughts or examples. Almost all of them are influenced, shaped, and refined by listening to other people. I thank ALL of them for their inspiring lectures and insightful examples which made this work possible. "],["some-preliminary-questions.html", "Chapter 1 Some Preliminary Questions", " Chapter 1 Some Preliminary Questions In your introductory statistics courses you have learned several fundemental ideas in statistics. It is now time to revisit them and see how well you have understood them. The following questions will help us do it. Let’s dive in. What is statistical inference? What is a parameter? What is a statistic? What is a ‘good’ sample? What is a sampling distribution? What is a confidence interval? How do you interpret a 95% confidence interval? What is the true meaning of ‘95%’? Hint: think about the concept of capture rate What is the Central Limit Theorem? What is the role of the Central Limit Theorem in the construction of a confidence interval? "],["sampling-distributions.html", "Chapter 2 Sampling Distributions 2.1 How to Construct a Sampling Distribution? 2.2 Some Questions to Ponder", " Chapter 2 Sampling Distributions Many statistical problems require us to estimate population parameters or model parameters. For example, we might want to know the percentage of binge drinkers in a college campus. A good point estimate for this parameter is the sample proportion \\(\\hat p\\). However, this value does not mean much unless we provide the variability of it. That is, we’d like to know if we take a another sample of the same size (from the same population), how different of a \\(\\hat p\\) would we see compared to the one we had before. In other words, how much variance can we expect in \\(\\hat p\\) from sample to sample. Let’s look at an example. Consider the following simulated population. Although this is a simulated population it is not that uncommon. For example, most service time distributions, like time to complete a transaction, time to repair a car, etc follows this pattern. Suppose we want to estimate the mean of this population using only a random sample of say, \\(n=30\\). The following code will do just that. mysample &lt;- sample(mypop, size = 30) mean(mysample) ## [1] 18.79358 What if we take another sample and calculate the mean. It will be different from the one above. mysample &lt;- sample(mypop, size = 30) mean(mysample) ## [1] 19.73084 This variability in our sample statistics is what we are going to study in this chapter. In particular, we are going to look at the variability of two sample statistics: The sample mean (\\(\\bar x\\)) The sample maximum. 2.1 How to Construct a Sampling Distribution? Take a random sample from this population and calculate (and store) the statistic we want. Then take another random sample of the same size as above and calculate (and store) the statistic we want. Repeat this process a large number of times. This will give us a collection of values for the statistic. For example, if we studying the sample mean \\(\\bar x\\) we will have a bunch of sample means corresponding to each of the random samples. Plot these sample statistics in a histogram. This will give us a graphical representation of the variability of the statistic. The following dataset contains a bunch (10000 to be exact) of random samples of size \\(n=30\\) from the above service time population along with some sample statistics calculated for each random sample. service_time &lt;- read.csv(file = &#39;https://raw.githubusercontent.com/gitcnk/Data/master/Service_Time_n30.csv&#39;) Let’s plot the distribution of the following sample statistics: The Sample Mean (\\(\\bar x\\)) The Sample Maximum. These distributions (above) are called sampling distributions. In particular, the first one is the sampling distribution of the sample mean and the second is the sampling distribution of the sample maximum. Therefore we can define a sampling distribution of a statistic as follows. Sampling Distribution of a sample statistic is defined as the distribution of values of that statistic under repeated sampling. Note that the key phrase of the above definition: ‘Sampling’. It tells us that this has something to with samples coming from a population. The next term is ‘Distribution’. It tells us how the sample statistic chages from sample to sample. Always remember to think about this twp terms carefully for few seconds before you answer any question about a sample statistic. Another important thing to remember is that the sampling distribution always depends on the size of the sample (\\(n\\)) we draw from the population. This is a crucial fact. For example, the above two sampling distributions are created from samples of size \\(n=30\\). Always mention this fact (sample size) when you describe a sampling distribution. To put this idea in perspective, let’s compare the two sampling distrubutions of \\(\\bar x\\) and sample maximum for samples of size \\(n=100\\). Do you see anything special? You’ll notice the following about the sample mean: Shape “Center” or typical value Spread n = 30 slightly skewed around 20 spans 20 units n = 100 quite normal very close to 20 spans 10 units You’ll notice the following about the sample maximum: Shape “Center” or typical value Spread n = 30 skewed around 80 spans 100 units n = 100 skewed around 100 spans 80 units Why are these numbers useful to us? They tell us whether our statistic, on average, is “close” to the population parameter. They help us to determine, how likely are we going to be very far from the target (population parameter) In the case of the sample mean we see that our estimates are quite close to the true value (\\(\\mu = 20\\)). And the likelihood that we are off too much decreases rapidly as \\(n\\) increases (from 30 to 100). In contrast, the sample maximum is biased (or underestimates) the true value (population max = around 150, technically this value is \\(\\infty\\)) and the likelihood that we are off does not decrease that much as \\(n\\) increases (from 30 to 100). These two properties - the “center” or the typical value of the statistic - the “spread” of the statistic and its relationship to sample size \\(n\\). is defined more technically as follows. Center of the sampling distribution It is defined as the expected value of the statistic. It is a measure we use to see whether our statistic, on average, hits the target (population parameter). Here is an example: Consider the sample mean \\(\\bar x = \\frac{\\sum_{i=1}^n x_i}{n}\\). The expected value of \\(\\bar x\\) is \\(E(\\bar x) = E\\left[\\frac{\\sum_{i=1}^n x_i}{n}\\right] = \\mu\\). Can you think how I got this value? The standard Error(variablity of the sampling distribution) is defined as the standard deviation of the statistic. Here is an example. Consider the sample mean \\(\\bar x = \\frac{\\sum_{i=1}^n x_i}{n}\\). The SE of \\(\\bar x\\) is \\(\\sigma/\\sqrt n\\), where \\(\\sigma\\) is the population standard deviation. Can you think how I got this value? Calculating the expected value and standard error in the above example is not that difficult. You will be doing this as a homework assignment for this chapter. The key is to find the variance of \\(\\bar x\\). That is, find \\(V(\\bar x)\\) first. Then take the square root of it to find the SE. All the plot and the definitions look very good, except that we have a major problem in our hand. The next section will expore those issues. 2.2 Some Questions to Ponder 1. Is it practically possible to know the shape of the sampling distribution of a statistic? Answer - PART 1: In general, no. Why? Think about the process of constructing a sampling distribution. It require us to draw a LOT of samples from a population. Now imagine the time, energy and the costs associated with this process in real life. For example, suppose you want to know the sampling distribution of the average commute times in NYC for samples of size \\(n=30\\). You’ll have to visit the subway stations (randomly) and interview 30 people (randomly) to gather information about their commute times. Then, you need to do this again and again for about 10,000 times! This is highly impractical and time consuming. In fact, if you think about it, it is a complete waste of your time and resources. Because, you could have simply taken a large sample of size 30,000 instead of 10,000 samples of size 30! Now you might wonder how do we even attempt to find this sampling distrubution. That’s the second part of the answer. Answer - PART 2: In some special cases we can find it. How? Long time ago (a very long time ago), statisticians have figured out that the statistics like the sample mean (averages in general), have a sampling ditribution which is normally distributed under some conditions. This fact is one of the celebrated theorems in statistics. It is called the Central Limit Theorem. It says that the sample mean \\(\\bar x\\) follows a normal model with center beign at the true population mean. More precisely, \\[ \\bar x \\sim N(\\mu , \\frac{\\sigma}{\\sqrt n})\\] We will be studying this in Chapter 3. Note: There are other statistics whose sampling distributions can be found usingtheoretical tools that are beyond the scope of this class. Math 351 and Math 352 is where we study them. 2. Is it possible to calculate the center (expected value), and standard error (SE) of a sampling distribution? Answer: In some cases, yes, we can find it. For example, we already found this for \\(\\bar x\\). As for the sample maximum , we can find the expected value and SE for the with some other tools that are beyond the scoope of this class. In some cases, it is imposible to do this. This leads us to the final question. 3. Is there a reasonable way to get some sense about the shape, center, and spread of the sampling distribution of a statistics? Answer: Yes. We appeal to a more modern method called the bootstrap method. We are going to learn this in Chapter 4. "],["the-infamous-n-30-rule.html", "Chapter 3 The Infamous ‘n &gt; 30 rule’ 3.1 Central Limit Theorem 3.2 Explorations with Real Data 3.3 Some Questions to Ponder", " Chapter 3 The Infamous ‘n &gt; 30 rule’ The title of this chapter may be a bit confusing. The formal title should be “The Central Limit Theorem”. But I wanted to make a point in this chapter about the misuses of this remarkable theorem. Let’s dive in. 3.1 Central Limit Theorem Before we formally introduce this theorem, let us first look at a motivating example. Recall the service time example we saw in Chapter 2. In that, we saw that the sampling distribution of \\(\\bar x\\) tend to follow a normal model as \\(n\\) increases while the sampling distribution of the sample maximum does not show any normal behavior. To put things in perspective, let us look at some plots. Sampling Distributions of the Sample Mean Carefully examine the plots above. In particular, pay attention to the following: shape spread (look at the scale of the x-axis) The behavior of the sample mean as \\(n\\) increases has two noteworthy aspects. For larger samples, like \\(n &gt; 100\\), the sampling distribution of \\(\\bar x\\) is normal with the “center” being very close to the true mean of the population that we sample from. The spread get’s smaller as \\(n\\) increases. Why is this important to us? First, the normal distribution is something that is easy to understand and we know how to calculate probabilities using the normal model. Also, recall the “68-95-99.7% rule” which describes how the probabilities change with respect to the standard deviation (spread) of the model. So, anything that follows a normal model is good news for the statistician, because we know a LOT about this model. Second, the spread (standard deviation) decreases as \\(n\\) increases. This is encouraging because it ensures that for larger samples we are not too far off from the “center” of the distribution which happens to be very close to the true mean. The following plot highlights how the sampling distribution concentrates around the true mean by ploting all 4 plots in the same x-scale. In fact, These facts were discovered a long time ago and they are summarized in one of the celebrated theorems in statistics. It is called the Central Limit Theorem. It says, under some conditions, the sample mean \\(\\bar x\\) follows a normal model with center being at the true population mean and the spread decreases at a rate of \\(1/\\sqrt n\\). We can denote this more succinctly as follows: \\[ \\bar x \\sim N(\\mu , \\frac{\\sigma}{\\sqrt n})\\] We will explore this theorem further in this chapter. But, let us first look at an example ( a statistic) that do not agree with the Central Limit Theorem. This example helps us to understand the core idea of this theorem. The plots below are sampling distributions of the sample maximum constructed from the same service time population (Example 2.1). Carefully look at the shape and spread of these plots. Sampling Distributions of the Sample Maximum You’ll notice immediately that the shape does not look normal even for \\(n=300\\). Also, the spread does not decrease that much. In fact, the spread is fairly constant across all 4 distributions. This is NOT an accident. The reason that this statistic, the sample maximum, does not obey the Central Limit Theorem is because it is NOT an average constructed from the sample. This is the core idea of this theorem. The normal behavior of the sampling distribution is ONLY applicable to sample averages. Here is another example to demonstrate this. Suppose we want to know the percentage of binge drinkers in college campuses. A good point estimate for this parameter is the sample proportion \\(\\hat p\\). What is the sampling distribution of this statistic \\(\\hat p\\)? Let’s create a small simulation to find this out. Here are the steps: Create a population of binge and non-binge drinkers Draw a sample from this population, say of size \\(n=30\\) and calculate \\(\\hat p\\). Repeat the above step (sampling and calculation of \\(\\hat p\\)) for a large number of times and plot the distribution of those \\(\\hat p\\) values. population_size &lt;- 1E6 sample_size &lt;- 20 my_drinking_pop &lt;- rbinom(population_size, 1, prob = 0.20) simulated_samples &lt;- 1E3 phat &lt;- 0 #storage bucket for(i in 1:simulated_samples) { mysample &lt;- sample(my_drinking_pop, size = sample_size) phat[i] &lt;- sum(mysample)/sample_size } ggplot() + geom_histogram(mapping = aes(x = phat), bins = 13) + labs( title = &#39;Sampling Distribution of phat&#39;, subtitle = &#39;Sample size n = 20&#39;, x = &#39;phat&#39;) As you can see, for \\(n=20\\) the sampling distribution of \\(\\hat p\\) is somewhat skewed. We can increase the sample size and see what happens to the sampling distribution. The following plots shows relationship of \\(n\\) with the shape and spread of the sampling distribution. It seems like the Central Limit Theorem is at play. That is, the sampling distribution of \\(\\hat p\\) looks normal for large \\(n\\). But, we know that the theorem only applies to AVERAGES. Now you might wonder is \\(\\hat p\\) an average? The answer is ‘Yes’. It is a proper average. It does not look like one, but we can show why it is an average. Let’s denote a random sample of binge grinkers as \\(x_1, x_2, \\ldots, x_n\\). Each \\(x_i\\) is either a \\(1\\) or \\(0\\), depending on whether the person is a binge drinker or not. Now if we write out the formula for the sample proportion \\(\\hat p\\) you’ll see why it is an average. \\[ \\begin{array}{ll} \\hat p &amp;= \\frac{Number \\ of \\ binge \\ drinkers \\ in\\ the\\ sample}{Total\\ number\\ of\\ people\\ in\\ the\\ sample} \\\\ &amp;= \\frac{\\sum_{i=1}^nx_i}{n} \\end{array} \\] As shown, above, \\(\\hat p\\) is an average and that’s why the sampling distribution of \\(\\hat p\\) behaves according to the Central Limit Theorem (CLT). It is now time to take stock of the important facts that we observed so far. Consider the following summary table: Statistic Obeys CLT Shape at n = 30 Shape at n = 50 Sample Mean (\\(\\bar x\\)) Yes Skewed Skewed Sample Max No not relevant not relevant Sample proportion (\\(\\hat p\\)) Yes Skewed Skewed Now you probably see why I labeled this chapter as “The infamous n &gt; 30 rule”. Most people believe that we can make use of the CLT if the sample size is “larger than 30”. But, as you saw in the above examples, this “rule” is extremely questionable. You might object to this observation by saying: “Well, you used simulated data. You could have cherry-picked your data to”prove\" a point\". Certainly, this is a valid (and reasonable) objection. Let us therefore look into some real datasets. 3.2 Explorations with Real Data In this section, we will look at 3 examples with real data. Keep in mind, these examples are hard to find. Because, it is very unlikely that we have ALL the members (data) available from a population. If we have all data points from a population, there is no need for statistical inference! These examples are chosen to highlight one of the main misconceptions of CLT, namely, the ‘\\(n&gt;30\\) rule’ (the title of this chapter). In each example, when you look at the sampling distribution of the mean, pay close attention to the sample size. Ask yourself, at what sample size does the sampling distribution start to look more like a normal model. NBA Player Salaries Example 3.1 NBA player salaries in 2016 season (Kolby Bryant’s last season). As with any other salary distrubution, these values are skewed with few players earning a LOT. The following plots show the population distribution of NBA player salaries in 2016. The following plots show the sampling distributions are made from samples from this population. Observations The first thing you will notice that at \\(n=30\\), the skewness is quite visible. At \\(n=50\\), it is still slightly skewed. So may be we ougth to modify the “rule” as \\(n&gt;50\\)? Undergraduates in US Colleges and Universities The second example is from ALL US colleges and universities. Note that there are about 5000 colleges and universities in the US. However, there are many missing datapoints since it is hard to find all data from all institutions. As a result the population size is 1269. In this example, we are intrested in the undergraduate population in US colleges and universities. Here is the population ditribution. The following plots show the sampling distributions are made from random samples from the above population. Even this example, at \\(n=30\\) the skewness is quite visible. At \\(n=50\\) the skewness is still there. Even at \\(n=100\\), if you look at carefully you’ll see that there is a slight skewness. So may be the rule should be \\(n&gt;100\\)? Departure Delays in LaGauadia Airport Now, let’s look at the final example. This data is about fligth delays. You can find a lot of information and dowload data for past years from this website: https://www.transtats.bts.gov/ONTIME/Departures.aspx The dataset we are looking at consists of ALL flights that departed from LaGuadia airport in 2017 from the three main carriers (American Airlines, Delta, and United). That is, all depatures from Jan 1 2017 to Dec 31 2017. The variable of interest is departure delay time. First, let’s take a look at the population distribution of departure delays. Now let’s look at the corresponding sampling distribution for the sample mean. You can clearly see that none of the above plots look normally distributed. Let us increase the sample size even further and see at what point it starts to look normal. The above plots underscore the main point of this chapter. The common belief that if \\(n&gt;30\\) the CLT is applicable for sample means. This is a very miguided notion. The above plots demonstrate that for this dataset the sample size \\(n\\) should be around 1000(!) to see a symmetric, bell shaped sampling distribution. By now, hopefully, you are convinced that there is a problem in the common understanding of the Central Limit Theorem. The main point in this chapter is to be aware of this issue and be very cautious in using the CLT. Run the following command in RStudio to open the data and do this exlopration yourself. It will open an app that allows you to change the sample size and construct sampling distributions on your own. library(shiny) runGitHub(repo = &quot;gitcnk/Apps/&quot;, subdir=&#39;CLT_NBA&#39;) library(shiny) runGitHub(repo = &quot;gitcnk/Apps/&quot;, subdir=&#39;CLT_Colleges&#39;) 3.3 Some Questions to Ponder 1. Why do people advocate for the &quot;n &gt; 30 rule&quot; if it is problematic? Answer: Hard to say. My guess is that this is similar to our beliefs and pratices of recycling plastics. That is, we were told to toss our plastics into the recycling bin and we normally believe that those plastics will get recycled somehow. By the reality is MUCH more complicated than that. Here is a wonderful documentary: https://www.youtube.com/watch?v=-dk3NOEgX7o Similarly, the Central Limit Theorem should be used with caution. There are many factors that we need to look into before we jump in and use this theorem. The issue mainly lies in understanding the population and its distribution. Since we don’t have access to the population (if we did we’ll be all be at the beach!), we have to rely on a random sample to make a judgement about the variability, skewness and outlier in the population. This is a very challenging task. For example, recall the flight delay example. If we haven’t had the entire dataset with us, would we have guessed that the population may look exremely skewed? Would we have guessed that there might be flights that are delayed 20 hours! Consider an example about cancer medication and the survial time. Unless we have an indepth knowledge about the cancer and the drug, it would be very difficult to even have a vague idea about the popluation distribution. In this case we need to rely on domain experts to tell us more about variance, skewness and extreme values. In realilty we only have a single sample from a given population and we need to make a judgement call on whether CLT is an appropriate technique to use with this sample. That’s why it is better to have a healthy level of skepticim about our data before we proceed. As good statisticians, it is our duty to inform our clients about the limitations of the data and the infrences that we draw from them. 2. What are some of the signs that CLT may be questionble? Answer: If you look back the simulated data examples and real data examples you’ll see that, for skewed data, it takes much larger sample size (in some cases in the 100’s) to use the CLT. Also, in the final example with departure delyas, the popualtion was not only highly skewed but also contained extremely large values in the tail. This is definitely a red flag. So if you suspect that the population is skewed and/or with extreme values you need to be super careful with the CLT. 3. How can we know whether the population is skewed or not when we don&#39;t have access to the entire population? Answer: This one is tricky. Yes, we’ll never have access to the entire population. All we have is a single random sample. If our sampling process was good in capturing the variance in the population then it will provide important clues about the skeness and precence of extreme values in the popualtion. For example, here are two random sample of sizes \\(n=30\\) and \\(n=100\\) respectively from the flight delay example. As you can see from the above two plots, the bigger sample was able able to capture more of the variance in the poulation but still failed to include some of the really large delaya times even with \\(n=100\\). But, this is where the statisticians need to step in. We can raise a red flag agaist anyone who is tempted to use the CLT with a sample like this for hypothesis testing or confidence intervals. We can educate them to see the danger of using CLT with a sample like this. 4. If we see extreme values (like in the above two samples) should we remove them and proceed to use the CLT? Answer: The safe answer is ‘no’. Removing data points has to be done with extreme care. There are some instances where removing extreme values may be legitimate. If the data values are recoreded incorrectly (errors in the data). For example, someone might have keyed in 10,000 (incorrect) instead of 1000 (correct). If we know for sure that this is the case, first we should try to find the correct value, if not remove it. If the purpose of the analysis dictates the removal of extreme values. Can you think of a situation where it might be essential to remove extreme values? This will be a part of your HW for this chapter. "],["the-bootstrap-method.html", "Chapter 4 The Bootstrap Method 4.1 Bootstrap Sampling Distribution 4.2 Bootstrap Confidence Intervals 4.3 Things to Ponder", " Chapter 4 The Bootstrap Method In short, the Bootstrap is a technique that we can use to get an idea about the sampling distribution of a statistic. It helps us to calculate the variability (standard error) of that statistic and also give us important clues about the shape of its TRUE sampling distribution. In addition, the Bootstrap method is a easy to use alternative to construct confidence intervals for population parameters especially when classical theory (which based on the Central Limit Theorem) is questionable or does not apply. In summary, it is another (useful) tool in our toolbox. But, it is NOT necessarily a `better’ tool. It depends on what we mean by ‘better’. In this chapter we will explore the pros and cons of this new method. Let’s dive it. 4.1 Bootstrap Sampling Distribution Example 4.1 Recall the service time distribution in Example 2.1. Service times to complete a task like repir a car, restaurent take-out order, etc. typically have a distribution of this type. What we normally see is a sample from the above population which may look like this: The sample mean is: mean(mysample) ## [1] 22.04726 But, we’d like to know how accurate this estimate is. That is, we would like to know the standard error. In this case, the sample statistic is quite simple. It is the sample average \\(\\bar x\\). We know how to find the standard error of this statistics using the \\(Var(\\cdot)\\) operator (Refer to you HW in Chapter 2). Here is the answer: sd(mysample)/sqrt(n) ## [1] 4.016029 We also learned in Chapter 2 that the standard error is nothing but the spread (std. dev) of the sampling distribution of \\(\\bar x\\). Is there a way to find the sampling distribution of \\(\\bar x\\)? Unfortunately, as we discussed in Chapter 2, it is not possible to find the sampling distribution of a statistic. This is where the bootstrap method comes in handy. Here is how we use it. What we have: A single random sample from the population. In this case a sample of size \\(n=30\\). What we assume: This sample is a good enough representation of the population. In other words, the shape and spread of this sample is similar to the population. We then proceed as if this sample is our “population”. The rest of the steps is almost identical to how we build the sampling distribution of \\(\\bar x\\) in Chapter 2. Take a random sample of size \\(n\\) (30 in this case) from this sample with replacement and calculate (and store) the statistic we want. Doing the sampling with replacement is the crucial point here. If we did not replace the data points, we will end up with the sample. For example, if our sample is (1,2,3), a resample with replacement looks like this: (2,2,3). Another resample might be (1,1,1). Since the sampling is done with replacement, we call these new samples bootstrap samples. Then take another bootstrap random sample of the same size as above and calculate (and store) the statistic we want. Repeat this process a large number of times. This will give us a collection of values for the statistic. We call these ‘bootstrap sample statistics’. For example, if we studying the sample mean \\(\\bar x\\) we will have a bunch of bootstrap sample means corresponding to each of the bootstrap samples. Plot these bootstrap sample statistics in a histogram. This will give us a graphical representation of the variance of the statistic. The following plot shows the bootstrap sampling distribution of \\(\\bar x\\) constructed out of the above random sample. One of the main reasons that we use the Bootstrap method is to get an idea of the uncertainty of our sample mean (\\(\\bar x\\)). In our case, the sample mean was 22.01. But we wanted to find the uncertainty of this estimate which is nothing but the spread of the true sampling distribution of \\(\\bar x\\). Since the bootstrap sampling distribution is an approximation of the true sampling distribution, we can calculate the spread of the bootstrap sampling distribution to get an estimate of the true standard error. As the table below shows, this method is extremely accurate. True SE Bootstrap SE 4.01 3.95 Copy and paste the following R commands in R Studio to open an app to experiment with this data so that you can convince yourself that the Bootstrap SE is a reasonable estimate of the true SE. library(shiny) runGitHub(repo = &quot;gitcnk/Apps/&quot;, subdir=&#39;BootstrapSE&#39;) After experimenting with the app you must be (hopefully) convinced that the Bootstrap method provides an alternative to find the standard error of a statistic. But, you might wonder why do we need an alternative when we already know how to find it theoretically. The reason is, sometimes (in fact in many real life cases), the statistic we are interested in is a more complicated one. Consider the following example: Example 4.2 Suppose you are interested in the correlation between amount of sleep and GPA. The sample correlation coefficient (\\(r\\)) for this data is 0.88. As before, we would like to know the standard error (SE) of this statistic. The sample correlation coefficient is defined as \\[ r = \\frac{\\sum_{i=1}^n (x_i - \\bar x)(y_i - \\bar y) }{sd_x sd_y}\\] It is virtually impossible to find the variance (\\(Var[ \\ r\\ ]\\)) of the above expression. Note that ratios of random variables are notoriously difficult to handle. But we can use the Bootstrap method to find an estimate of the SE using the Bootstrap sampling distribution of the correlation coefficient, \\(r\\). The following plot shows the Bootstrap sampling distribution of \\(r\\) from the above dataset. Based on this analysis we can say the following about the sleep v. GPA data: True correlation Sample correlation (\\(r\\)) True Standard Error Bootstrap Standard Error We’ll never know 0.88 Very Difficult to find 0.035 One thing to stress here is that, we would never know the true correlation or its variance. However, the above table provides a better understanding of what it might be. It tells us that the sample correlation is 0.88 but the uncertainly is on average, about 0.035 units. Hopefully, this example will convince you to believe the utility of the Bootstrap method as technique to find the standard error of any statistic. Simply create the bootstrap sampling distribution and find its spread (standard deviation). To drive this point, let us look at another example. Example: This was a study on in-hospital deaths from mayocardial infarction with ST elevation (STEMI) which was done in Oregon. Summary stats of the study is as follows: 9 hospitals in the Providence Health system in Oregon. Study period : from 2002 to 2003. 913 STEMI patients treated, of whom 105 died (in-hospital deaths) from mayocardial infarction with ST elevation (STEMI). Each patient was assigned a Thrombosis in Mayocardial Infarction Risk Scores The following table contains data grouped by risk score. In addition, information about the national mortality rate for each risk category is also given. Risk_score Patients Deaths NRMI_Mortality_in_percents 0 34 0 0.17 1 75 0 0.67 2 88 1 1.70 3 73 1 2.88 4 91 4 5.32 5 110 11 9.43 6 94 19 14.12 7 70 19 18.86 8 51 8 19.93 9 47 12 24.30 10 31 6 26.22 11–14 21 7 33.42 No score 128 17 14.42 Overall Question: How “good” is this hospital system (in terms of death rates) compared to the national level? How could we begin to answer this question? You might suggest to look at the expected deaths IF the mortality rate for the hospital system is the same as the national rate. For example, for risk level 5, the expected number of deaths is \\(110*9.43\\% \\ = 10.37\\), which is not that different from the actual death count of 11. If we do this for all risk levels we get a column of expected deaths. From that we can look at the difference between the observed and the expected deaths. But, there is a problem in this approach. Consider the following two cases: Case 1 Case 2 Observed = 10 Observed = 100 Expected = 5 Expected = 95 In both cases, the difference is 5, but we can all agree that a deviation of 5 is more severe in Case 1 than it is in Case 2. A better approach is to look at the ratio between the observed and the expected counts as shown below: Risk_score Deaths Expected.Deaths OEratio 0 0 0.058 0.000 1 0 0.503 0.000 2 1 1.496 0.668 3 1 2.102 0.476 4 4 4.841 0.826 5 11 10.373 1.060 6 19 13.273 1.431 7 19 13.202 1.439 8 8 10.164 0.787 9 12 11.421 1.051 10 6 8.128 0.738 11–14 7 7.018 0.997 No score 17 18.458 0.921 If the death rates in this hospital system is about the same as the national rate, then we would expect these ratios to be close to 1 and hence the average of these difference should be close to 1. For this data, the mean of these ratios is 0.8 (0.7997 to be exact). Since this value is less than one, it is good news for the hospital system. It implies that they are doing better compared to the nation. But we need to know the uncertainty (standard error) of this estimate. Calculating the exact standard error of this ratio is quite difficult. However, we can use the bootstrap to come up with a reasonable estimate of the standard error. As before, we’ll resample the 13 ratios with replacement and build the bootstrap sampling distribution. Then we measure the spread (standard deviation) of this bootstrap distribution to find the standard error of the ratio. The spread of the bootstrap sampling distribution (standard deviation) is 0.1189. Combining this with the mean ratio of 0.8, we can say that the death rate of this hospital system is 0.8 times the national rate, with an uncertainty of 0.12. This is encouraging news to the hospital system. In addition, you can see from the above plot, a large number of the bootstrap ratios are less than one which suggests that the TRUE death rate would probably be less than 1 as well. But we don’t quite know that. That is the kind of inference we would like to draw. We will get to that in the next section. But before that we need take stock of what we’ve learned so far. Brief Summary Example 1: Established that we can use the bootstrap method to find the standard error of a statistic. Example 2 and 3: Discussed how the bootstrap method help us in finding standard errors of complicated statistic, like the correlation coefficient and unfamiliar ratios of random variables. 4.2 Bootstrap Confidence Intervals As alluded at the end of section 4.1, we would like to make inferences about population parameters like the TRUE(population) mean or the TRUE correlation or TRUE death rate ratio. In classical statistics, we did this using confidence intervals. For example, we can use the random sample in example 1 (service time) to build a confidence interval for the TRUE average service time. In classical statistics, we do this using the following formula: \\[ \\bar x \\ \\pm \\ t_{score} \\ . SE(\\bar x) \\] \\[ \\bar x \\ \\pm \\ t_{score} \\ . \\frac{S}{\\sqrt n} \\] where \\(S\\) is the sample standard deviation. The final interval can be easily calculated using R as follows: t.test( x = mysample)$conf.int ## [1] 13.83355 30.26096 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 Now let us see how to use the Bootstrap method to construct a confidence interval from the same sample. There are 3 main approaches, but we are going to learn only two of them. The other is beyond the scope of undergraduate work. Bootstrap SE Plug-in Method As the name implies, we simply use the bootstrap SE and plug that value in as an estimate for the actual Standard error of the statistic. For the above example, we can use the bootstrap estimate of SE, which was 3.95, and compute the confidence interval. This method works well if the bootstrap sampling distribution is roughly normal. For other cases, it will not produce good results. Therefore, this method is not very helpful with more complicated statistics. Bootstrap Percentile Method The second method is more flexible, in that, it does not require the bootstrap sampling distribution to be normal. If we are calculating a 95% confidence interval, we simply find the \\(2.5^{th}\\) and the \\(97.5^{th}\\) percentiles of the bootstrap sampling distribution and those are our lower and upper confidence limits. For example, for the service time data, the confidence bounds are shown in the figure below. The 95% bootstrap confidence interval for the TRUE average service time (\\(\\mu\\)) is [14.74, 30.17]. The corresponding 95% classical confidence interval is [13.83, 30.26]. As you can see, the two intervals do not differ much. Can you think what the reason might be? The real question that you already might be asking in your head is: “Why do we need these bootstrap confidence intervals when we already know how to find confidence intervals based on CLT?” As in the case of calculating standard errors, the real benefit of this method comes when we DO NOT have classical confidence intervals or when it is extremely difficult to find one. Consider the correlation example we looked at (Example 4.2). There is no easy way to find a confidence interval for the TRUE correlation (\\(\\rho\\)). But the bootstrap provides an easy alternative. The following plot shows the confidence bounds calculated from the correlation data. The 95% bootstrap confidence interval is [0.8 , 0.94]. As you can see in the above plot, the bootstrap sampling distribution is quite skewed. Yet we can use the bootstrap percentile method to find a 95% confidence interval for the unknown parameter. This is another benefit of bootstrap confidence intervals. They do not require that the sampling distribution of the sample statistic being normal. As you recall, normality of the sampling distribution is a crucial assumption in classical confidence intervals that are based on the central limit theorem. To wrap up this section, let us calculate a 95% bootstrap confidence interval for the death rate data. The following plot shows the confidence bounds. The 95% bootstrap confidence interval is for the TRUE observed/expected death ratio is [0.56 , 1.03]. This is very important information for the hospital system. If you recall, the mean sample ratio was 0.8, which suggested that this hospital system is doing “better” compared to the national rate. But the confidence interval gave us a better understanding of the situation. Based on this interval, you can conclude that the this hospital system is not that different from the nation in terms of heart disease deaths. The apparent 0.8 (‘better’ performance) is a result of chance. Can you think how I arrived at this conclusion? That would be part of your HW :) 4.3 Things to Ponder Can we trust bootstrap confidence intervals? The short answer is YES, we can trust them (otherwise, why use it, right?). The only caveat is that the original sample should be a REPRESENTATIVE sample of the population. Without representativeness, we can’t do anything bootstrap or classical. This is a very important point that we (statisticians) need to keep in our mind. No matter how fancy your models and tools are, if your data is ‘garbage’, you end up with ‘garbage’ answers. Are there instances where bootstrap confidence intervals fail to live up to the true capture rate? Yes. Just like with classical confidence intervals, the bootstrap confidence intervals may fail to achieve the advertised capture rate. It can happen due to many reasons. One of the main ones is that the original sample is of poor quality (biased or non-representative). There are ways to improve these intervals but those modifications are beyond the scope of this course. What other benefits (other than calculating standard errors of statistics and constructing a confidence intervals) do we get from the bootstrap method? The most important one is the bootstrap sampling distribution. We get a visual representation of the shape and center and variability of the statistic. So why is it useful? Recall that we have no way of knowing the TRUE sampling distribution, and the bootstrap sampling distribution provides a great alternative to visualize how the TRUE sampling distribution MIGHT look like. Here are few other things to keep in mind: Pros: It provides a way to get a sense about the sampling distribution of any statistic. It is a very flexible technique that can be used to construct confidence intervals for a variety of parameters. In some cases, like estimating a population correlation or some other complicated parameter, there are no easy classical methods to construct a confidence interval. The existing classical methods for such problems are based on bunch of assumptions and complicated statistical maneuvers. However, the bootstrap method provides a easy to use alternative to construct confidence intervals even for complicated parameters. In some cases, like estimating a population ratio (e.g. in the case of Heart disease data), we do not have any classical method to construct a confidence interval, so the bootstrap method is our ONLY hope. Cons: It requires more computational power than classical methods. In some cases we need to resample a LOT and it can take some time to produce the bootstrap sampling distribution of a complicated statistic (we haven’t done these in our class). It requires some coding skills to implement the bootstrap method. In our case we used the ‘mosaic’ package, which simplified lot of the work. In some cases, we have to develop our own code, which requires some coding skills. Theory behind the bootstrap method is complex. So it is hard to convince a reasonable person that this method actually works and not some voodoo science. :) "],["permutation-randomization-tests.html", "Chapter 5 Permutation (Randomization) Tests 5.1 Alternative to the Two Sample t-test 5.2 Application to Categorical Data 5.3 Application to Linear Regression 5.4 Things to ponder", " Chapter 5 Permutation (Randomization) Tests In Chapter 4, we looked at a new method (The Bootstrap) to find standard errors and confidence intervals. In this chapter we are going to learn a new method to perform hypothesis tests. These tests are called Permutation tests. Unlike classical hypothesis tests, these tests are NOT based on Central Limit Theorem. We will learn the basics of these tests and their limitations through 3 examples. Let’s dive in. 5.1 Alternative to the Two Sample t-test Example A sample of 16 healthy women aged 18 - 40 were randomly assigned to drink 24 ounces of either diet cola or water. Their urine was collected for three hours after ingestion of the beverage and calcium excretion (in mg.) was measured . The researchers were investigating whether people who drink diet cola tend to loose more calcium out of their system, which would increase the amount of calcium in the urine for diet cola drinkers. Ref: Larson, Amin, Olsen, and Poth, Effect of Diet Cola on Urine Calcium Excretion, Endocrine Reviews, 31[3]: June 2010. Here are the summary statistics for the cola experiment: Cola Water Sample Mean 56.00 mg 49.125 mg n 8 8 Now let’s look at the following dialog: You: The difference in means = 6.875 mg, so it APPEARS that the people who drank cola tend to lose more calcium than the people who drank water. Cola lobbyist: This is total nonsense! This difference is just a pure coincidence. You: You mean all of the high calcium values just happened to be in the cola group just by chance? Cola lobbyist: Hell yeah! Look, you and I both know that cola is just like water, right? You: No!! I don’t think so . But I’m willing to give you the benefit of the doubt for now, until we analyze the data. Cola lobbyist: OK, here’s what I mean. Look at the Cola group and the water group values: Cola: 50,62,48,55,58,61,58,56 Water: 48,46,54,45,53,46,53,48 If, for example, you exchange the 62 (in cola group) with the 45 in the water group the difference in means will be cola_mean=49.125 water_mean=51.25 49.125 - 51.25 = -2.125 !!!! See, I told you, would you now say that water is bad! My point is: some individuals naturally have high calcium in their urine so if the cola group happens to get one such individual like in the original sample it can inflate the mean. You: Not so fast, you just picked the best scenario favorable to your side. Why don’t we look at other possibilities. Cola lobbyist: What do you mean? You: Here’s what I mean. IF cola is the same as water, then we can simply start with 16 people, pick 8 at random and label them as cola group and the rest as water group. After all, according to you, cola is just like water, right? So any of these values can be just as likely to be in the cola group as in the water group. We can repeat this many times and calculate the mean difference for each random draw and see what are the likely values IF “cola is same as water” This is the main idea of the permutation tests. Under our null hypothesis, we can permute the group labels. In other words, the groups (in this case, cola and water) is not influential to the final outcome (in this case, calcium level). Let us look at the behavior of the difference in sample means if we were to permute the group labels. The way we perform this is quite simple. Step 1: Permute the group labels. Step 2: Calculate the sample means for the new groups and take the difference. Call this \\(d\\). Step 3: Repeat steps 1 and 2, and record all the mean differences, \\(d_i\\)’s, for \\(i=1,\\ldots , N\\), where \\(N\\) is a very large number. Step 4: Plot the permutation distribution of \\(d\\). Then we can see, how many permutations (scenarios) had a mean difference of 6.875 or higher. The following code and plot shows the permutation distribution for the cola experiment. observed_difference &lt;- diffmean( Calcium ~ Drink , data = mydata) PD &lt;- do(1000)*diffmean( Calcium ~ shuffle(Drink) , data = mydata) Cola_plot &lt;- ggplot(data = PD) + aes(x = diffmean) + geom_histogram() + geom_vline(xintercept = observed_difference, col=&#39;red&#39;, linetype = &#39;dashed&#39;) + labs(title = &#39;Does Cola increase calcium levels in urine?&#39;, subtitle = &#39;Data source: Endocrine Reviews, 31(3), June 2010.&#39;, x = &#39;Difference in means (Water - Calcium)&#39;) Cola_plot As you can see the observed difference in means is -6.875 (not that the value is negative because of order of subtraction, which is Water group - Calcium group ). This difference is quite unlikely IF cola has nothing to do with calcium levels. We can calculate how unlikely is this be means of a p-value. This p-value can be calculated as follows: Step 1: Count the number of permutations that gave rise to mean difference of -6.785 or less Step2 : Divide this count by the number of simulated scenarios. For the simulation we did, we found 10 scenarios with a mean difference of -6.875 or less which translates to a p-value of \\(10/1000 = 0.01\\). Remember, when you do your own simulation, your p-values would be slightly different. We can compare this p-value to the classical two sample t-test p-value. t.test( Calcium ~ Drink, data = mydata) The p-value from the t-test is 0.007, which is close to the permutation test p-value of 0.01. However, the t-test relies on the fact that the data comes from approximately normal population or the sample sizes are large enough to rely on the Central Limit Theorem. In this case, however, we can not be confident on either of these assumptions given the very small sample size. Therefore, the permutation test is a excellent alternative to bypass these kinds of strong assumptions and yet produce useful results. The only assumption we need to perform the permutation test is the exchangability of group labels under the null hypothesis. Let us look at another example to solidify this idea. 5.2 Application to Categorical Data This example is from a case in the U.S. District Court for the Southern District of New York (Waisome v. Port Authority of New York &amp; New Jersey). In 1991, Felix Waisome and a group of plaintiffs sued the port authority of New York claiming racial bias against Black officers in the promotion process, because, according to him, the port authority officials were passing black officers at a lower rate compared to the white officers. 508 White officers and 64 Black officers took an exam for promotion. 455 Whites officers and 50 Black officers passed the exam. The pass rate for whites (\\(\\hat p_{white} = 455/508 = 89.56\\%\\)) and the pass rate for blacks (\\(\\hat p_{black} = 50/64 = 78.13\\%\\)). On face value, it seems like the pass rate for whites is higher than that of blacks. As statisticians we know that this is not the real issue. The real question is: could this be due to chance? The standard method to answer this question is to use the two sample proportions tests. \\(H_0:\\) There is no difference in pass rates. \\(p_{white}\\) = \\(p_{black}\\) \\(H_1:\\) The pass rate for whites is higher than that of blacks: \\(p_{white}\\) &gt; \\(p_{black}\\) The p-value from the two sample proportions test can be calculated using the following R-command: prop.test( x = c(455,50), n = c(508,64), alternative = &#39;greater&#39;) The p-value from the above test is 0.006, which suggests that a difference of this magnitude (89.56% vs 78.13%) in pass rates could not have happened by chance IF the TRUE pass rates of whites and blacks were the same. This test relies on the Central Limit Theorem which require large sample sizes. For the black group the sample size is 64, which is not small but not that large either. Therefore, the p-value from the test may not be that accurate. Let us now use the permutation approach to find a p-value. The main issue in question is that the police department was bias in passing the candidates. In order to answer this, we will assume that they are not bias. Under this premise an officer passing the exam does not depend on his or her race. This is what allow us to permute the race labels among the candidates. That is, we shuffle the group label and recalculate the difference in pass rates. We repeat this for lots of shuffles and build the permutation distribution as follows: # Create the data frame using the counts from each group race_info &lt;- c(rep(&#39;W&#39;,508), rep(&#39;B&#39;,64)) exam_info &lt;- c(rep(1,455), rep(0,53), rep(1,50), rep(0,14)) exam_data &lt;- data.frame(Race=race_info, Pass=exam_info) observed_difference &lt;- 455/508 - 50/64 PD &lt;- do(10000)*diffprop(Pass ~ shuffle(Race), data = exam_data) PassFail_plot &lt;- ggplot(data = PD) + aes(x = diffprop) + geom_histogram(bins = 20) + geom_vline(xintercept = observed_difference, col=&#39;red&#39;, linetype = &#39;dashed&#39;) + labs(title = &#39;Is There Evidence of Racial Bias?&#39;, subtitle = &#39;Data source: Waisome v. Port Authority. U.S District Court (1991)&#39;, x = &#39;Difference in pass rates (white - black)&#39;) PassFail_plot As you can see in the above plot, IF the police department did not use race as a factor in passing the candidates, then it is unlikely that we observe difference of this magnitude (11.43% = 89.56% - 78.13%) in pass rates. To be precise, the observed p-value is .0086. This small p-value raises a flag about the unbiasedness of the testing procedure. Does it mean that the police department is culpable of racial bias? 5.3 Application to Linear Regression The final example of this chapter illustrates the flexibility of permutation tests. In this example, we will see how to apply this technique to test hypotheses about linear models. In particular, we will learn how to test a hypothesis about the slope coefficient in a simple linear regression model. The dataset we use here contain information about pizzas. You can access the dataset using the following command: Pizza &lt;- read.csv(file = &#39;https://raw.githubusercontent.com/gitcnk/Data/master/Pizza.csv&#39;) Suppose we are interested in how much calories a pizza has and its relationship to the amount of fat in the pizza. The following plot shows the this relationship: As you can see, the relationship is linear and quite strong. The slope of the fitted line is 6.52. How do we assess whether this value is statistically significant? The classical method is to look at the t-statistic corresponding to the slope coefficient. We can state describe the test and the test statistic as follows: \\(H_0\\) : Slope( \\(\\beta\\) ) = 0 \\(H_1\\) : \\(\\beta \\neq 0\\) Test Statistic : \\(\\frac{\\hat \\beta - 0}{SE(\\hat \\beta)}\\) You can easily calculate the p-value associated with this test using the following R-command: mymodel &lt;- lm(calories ~ fat , data = Pizza) summary(mymodel) The p-value you get for the slope test is essentially zero. What this means is, IF fat has nothing to do with the calories in a pizza, then it is highly unlikely that we observe a slope coefficient of 6.52 by chance alone. This seems logical given the strong correlation that we see in the scatter plot above. Now let us see how to use the permutation approach to calculate the p-value for the slope coefficient. First, we need to ask is there any group label that we can permute. Recall that in all previous examples in this chapter we worked with two groups. e.g. cola vs. water, whites vs. blacks. In this case, there is no such group variable. But we can still use the permutation method. The key is with the null hypothesis. According to the null hypothesis, we assume that there is no linear relationship between fat and calories. Therefore, any of the calorie values could have occurred with any of the fat levels. Let us look at the first five rows of the data. Fat Original Calorie Values Shuffled Calorie Values 15 364 332 11 334 307 12 332 364 14 341 334 9 307 341 The reason we can permute the calorie values is because the null hypothesis states that fat can calories are not correlated. Hence any of the calorie values is equally likely to have occurred with any of the fat levels. This is the key point that we exploit to build our permutation distribution of slope estimates. As shown above, we shuffle the calorie values and refit the linear model and calculate the slope estimate. We repeat this process a large number of times and for each shuffle, we estimate the slope parameter. The following code and the plot shows the output of this process. observed_slope &lt;- coef(lm(calories ~ fat, data = Pizza))[2] PD &lt;- do(10000)*lm(calories ~ shuffle(fat), data = Pizza) Slope_plot &lt;- ggplot(data = PD) + aes(x = fat) + geom_histogram() + geom_vline(xintercept = observed_slope, col=&#39;red&#39;, linetype = &#39;dashed&#39;) + labs(title = &#39;Permutation Plot of Slope values&#39;, subtitle = &#39;Data source: Watkins, et al. (2010)&#39;, x = &#39;Slope coefficient&#39;) Slope_plot As we expected, under the premise of no correlation with fat and calories (null hypothesis), we expect most of the slope values to be around zero and with some spread ranging from -2.5 t0 2.5. But the observed slope of 6.52 (red dashed line) is almost impossible to occur by chance alone if there is no correlation with fat and calories. Hence we reject our null hypothesis of slope = 0 and conclude that the TRUE slope is significantly different from zero. Summary Permutation tests are quite useful alternatives for classical tests when there are doubts about the assumptions behind those classical tests. We can use them in a variety of settings, two sample tests for means, two sample proportions, linear regression. The key for any permutation test is that the null hypothesis should indicate a situation where it allows us to shuffle (permute) the data. 5.4 Things to ponder 1. Can we use permutation tests in any situation? Answer No. There are many cases where we can not use permutation tests. For example, if we have single random sample and say that we want to test the TRUE mean \\(\\mu\\) = 10. There is nothing to permute and hence we can’t use the permutation method. 2. Are the permutation tests resistant to outliers? Answer No. The outliers may get shuffled around but theirs effects will still be there. Also, when we calculate the p-value using the observed value of the difference or slope which are based on the original data. As a result the effects of the outliers will not disappear. Therefore, as with any analysis, it is very important to deal with outliers appropriately. Reminds me of a quote from two of my heroes in statistics: “There is no substitute for facing the facts in detail.” - John Tukey and Fredrick Mosteller. "],["p-values-jelly-beans-and-mosquitoes.html", "Chapter 6 P-values, Jelly-Beans, and Mosquitoes 6.1 What is a p-value? 6.2 P-values and Large Samples. 6.3 Slicing and Dicing the Data 6.4 A Question to Ponder Summary", " Chapter 6 P-values, Jelly-Beans, and Mosquitoes The title of this chapter is rather strange. If you keep reading, you’ll see why I included the terms “Jelly Beans” and “Mosquitoes”. The main goal of this chapter is to better understand the concept of the p-value, its limitations and misinterpretations. Let’s dive in. 6.1 What is a p-value? The short answer is that it is a conditional probability. Then you might ask: “conditional on what?”. It is conditional on the null hypothesis. For example, suppose we are testing whether average heart rate of males is higher than that of females. \\(H_0:\\ \\mu_{male} = \\mu_{female}\\) \\(H_1:\\ \\mu_{male} &gt; \\mu_{female}\\) We collect data from two independent random samples. Say we have a sample mean difference of \\(\\bar x_{male} - \\bar x_{female} = 13\\) bpm (beats per minute). As good statisticians, first we ask the question: “Is this difference real or is it a result of chance?”. So we perform a two sample t test to answer this question. Suppose we got a really small p-value of .000001. How do we interpret this? Recall that the p-value is the probability of observing a sample mean difference of 13 (or more extreme) given the null hypothesis. That is: \\[ p-value = P[(\\bar X_{M} - \\bar X_{F} ) \\geq 13 \\ | \\ \\mu_{M} = \\mu_{F}] \\] In simple terms, all this is saying is, how likely it is to observe a sample mean difference of 13 (or more) IF males and females have similar heart rates. This is where the troubles start. Most people forget the last part of that statement. That is, they forget (or not aware of) the conditional nature of the p-value. As a result p-values get mischaracterized. Here are some examples: The p-value is the probability that the null hypothesis is true. 1 - (p-value) is the probability that the alternative hypothesis is true. If you carefully thought about the p-value, you’ll should see that neither of these statements is true. As mentioned earlier, this is a direct result of ignoring the conditional aspect of the p-value. Another issue that you’ll explore in your readings will be the “prosecutor’s fallacy”. This is also a quite common mischaracterization of p-values. Let’s take the same example above. The p-value was 0.000001 (1 in a million). Correct If males and females have the same heart rates, then a sample difference of 13 is almost impossible to occur (1 in a million). Incorrect With such a big sample difference of 13 beats per minute, the chance is very small (1 in a million) that males and females have the same heart rates. Do you see the slippery nature of p-values? Both statements look correct, but only one of them is actually correct. The next section discusses another strange aspect of the p-values. 6.2 P-values and Large Samples. As statisticians, we love large samples. The more information we have the better. However, we need be careful with the p-values that we get when we are dealing with very large samples. This goes at the heart of data analysis. That is, we should pay a LOT of attention to context when dealing with numbers. After all, data is simply numbers with a context. Let’s look at the following example: Example: Suppose you have two independent sample from two population and you wan to test the following hypothesis: \\(H_0:\\ \\mu_{1} = \\mu_{2}\\) \\(H_1:\\ \\mu_{1} &gt; \\mu_{2}\\) Before we jump in, let’s look at some descriptive statistics and plots about the two samples. The dataset can be accessed using the following link: large_data &lt;- read.csv(file = &#39;https://raw.githubusercontent.com/gitcnk/Data/master/very_large_sample.csv&#39;) As the following plot shows the two samples are almost identical both in shape and spread. Both samples have almost the same mean and median. Median Mean SD n group1 10.0197 10.0248 0.994 25000 group2 10.0025 10.0039 0.994 25000 The other important fact to observe is that the sample sizes are very large (25000 per group!). Now if we conduct a two sample t-test for mean difference, we get the p-value of 0.009. Normally, one would say: “there is statistically significant difference between the two group means”. Although, I would not recommend this type of language, I’m sure you may have seen statements like this in media and even in scientific publications. Along with this kind of language, comes the misunderstanding that the difference between the groups is important or practically meaningful. This is where the trouble is. Some people associate “statistical significance” with “practical significance”. What we ought to do is to estimate the magnitude of the TRUE difference between the groups and interpret that in relation to the context. A 95% confidence interval will give us a better idea of the TRUE mean difference, which in this case is [0.003, 0.038]. These values need to be put in context. For example, if the goal of the study is to find out whether the average heart rates of males and females are different, then a difference of 0.038 (upper confidence limit) beats per minutes is not practically significant although it is “statistically significant”. The main takeaway of this example is that when you have really large samples, tiny differences become statistically significant. So, if we are not careful in interpreting the results in the right way, we’ll end up with silly statements like “males and females have a statistically significant 0.038 bpm difference in heart rates!”. To better understand why tiny differences become statistically significant with large samples, let us look the form of the test statistic carefully. The general form of most test statistic (like the t-statistic) is: \\[\\frac{Estimate - Parameter }{Standard\\ Error}\\] For example, the t-statistic has the following form: \\[T = \\frac{\\bar X - \\mu }{\\frac{s}{\\sqrt n}}\\] where \\(\\mu\\) is the population mean and \\(s\\) is the sample standard deviation. What will happen to the T statistic if you have a large sample? How does that affect the p-value of the test, would the p-value increase or decrease? With some thought, and some rearranging of terms in the expression above, you’ll see the connection between the sample size \\(n\\) and the t-value and how it drives the corresponding p-value. 6.3 Slicing and Dicing the Data The next point that we want to look at is not a problem with p-values. It is a problem with us (the practitioners of statistics)! In practice, when we are working with datasets with may variables, it is quite tempting to perform lots of analyses hoping to find something interesting. For example, consider the heart rate example we discussed above. Your main goal was to see whether males and females have different average heart rates. Suppose you collected data from many individuals and in addition to their heart rates and gender you’ve collected other information like, race, hair color, eye color, income level (high, medium, low), education level, etc. You: Wait, why are you collecting all these other data?. Statistician: Well…, they are here for the test and we just wanted to get some background information. After all, it does not cost us or them anything to have this extra information. It is better to have more than less, right? You: I guess… This is where the trouble starts. Suppose you conduct the test and you did not find any difference between the two groups (males vs. females). If you are gung ho about your theory(hypothesis) that there exist some difference in heart rates between males and females, then you will be quite disappointed to see that the data says something contrary to your hypothesis. Rather than accept the this reality, you go on quest to “prove” it somehow. That is, you start torturing the data until it confess. What do I mean by this? This is where the extra variables you collected come in to play. For example, you can test: African American males vs African American females OR African American males vs White females OR . . . White males with dark hair and blue eyes with low education and in low income level vs. White females in with blond hair and green eyes with high education and in high income level. The choices are endless (and stupid!). This kind of subgroup testing is what I call slicing and dicing the data. When you perform a lot of tests like this, the likelihood of you finding a statistically significant p-value is very high. In fact, you can show (we’ll do it in class) that this probability grows exponentially with the number of different subgroup tests you perform. So you are bound to find “something” even if there is nothing there. The following cartoon illustrates this phenomena quite beautifully. The Jelly Bean Story: https://xkcd.com/882/ Another funny one: https://xkcd.com/1478/ Now you might say, “Well, this is a cooked-up example. No one in their right mind do such egregious things”. You are absolutely right. But the reality is entirely different from what we’d like to believe. The following article is a powerful example of this unfortunate practice. A recent example: Cornell scientist turned shoddy data into viral studies about what we eat: https://www.buzzfeednews.com/article/stephaniemlee/brian-wansink-cornell-p-hacking A Slightly Different Example To drive this point home about p-values are good but humans are not, consider the following regression model and its p-value for the slope coefficient. Can you think what is going on in this case? The data: mydata &lt;- read.csv(file = &#39;https://raw.githubusercontent.com/gitcnk/Data/master/FlightDelays2017.csv&#39;) Here is the plot: ## ## Call: ## lm(formula = Delay ~ FlightNo, data = mydata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -33.18 -17.39 -14.14 -6.35 678.82 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.450583 1.181966 7.996 1.67e-15 *** ## FlightNo 0.002765 0.001189 2.326 0.0201 * ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 41.61 on 4027 degrees of freedom ## Multiple R-squared: 0.001341, Adjusted R-squared: 0.001093 ## F-statistic: 5.408 on 1 and 4027 DF, p-value: 0.02009 Can you find what is wrong with the above analysis?\" 6.4 A Question to Ponder Should we not bother about statistically insignificant values? Answer: No. Again, it is the context that matters the most. The following example from the U.S. Supreme Court highlights this point quite effectively. The Supreme Court of the United States in March 2011 decided the case Matrixx Inc vs. Siracusano. The company Matrixx Inc. is the manufacturer of the cold medicine Zicam. They publicly dismissed reports linking Zicam and anosmia (loss of the sense of smell) when it had evidence of a biological link between ingredients of Zicam and anosmia. They used the term “statistically significant” to hide their misdeeds. They told a lower court that they did not disclose the adverse effects because the differences between Zicam users and others were not statistically significant. But the Supreme Court disagrees. In delivering the opinion for a unanimous Court, justice Sonia Sotomayor said: \"A lack of statistically significant data does not mean that medical experts have no reliable basis for inferring a causal link between a drug and adverse events. The FDA similarly does not limit the evidence it considers for purposes of assessing causation and taking regulatory action to statistically significant data. In assessing the safety risk posed by a product, the FDA considers factors such as “strength of the association,” “temporal relationship of product use and the event,” “consistency of findings across available data sources,” “evidence of a dose-response for the effect,” “biologic plausibility,”….. Assessing the materiality of adverse event reports requires consideration of the source, content, and context of the reports. This is not to say that statistical significance (or the lack thereof) is irrelevant–only that it is not dispositive of every case.\" More information: https://www.oyez.org/cases/2010/09-1156 Summary When dealing with really large samples, we need to be extra careful with the p-values. Why? Because, tiny differences become statistically significant and as a result we may be tempted to believe that there is something “important” with the data. Always pay attention to context and think about practical significance and don’t fall in love with statistical significance. The more tests you perform, you are bound to find “something”. Then people have the tendency to come up with a “cute story” to justify the results. As you can see, the issue is not with p-values, it is with us. The practitioners of statistics (us) are responsible for this mess. I would like to close this chapter with the following remarkable quote. It captures the essence of this chapter. “p-values are like mosquitoes. They have an evolutionary niche somewhere and no amount of scratching, swatting, and spraying will dislodge them.” – Campbell, J. P. (Editor): Journal of Applied Psychology, 67(6), 691–700. (1982) "],["chi-squared-test-and-its-variants.html", "Chapter 7 Chi-Squared Test and its Variants 7.1 Goodness of Fit Test 7.2 Test of Homogeneity 7.3 Test of Independence Summary", " Chapter 7 Chi-Squared Test and its Variants The Chi-Squared test is one of most popular hypothesis tests in applied statistics. Researchers from many disciplines use this test in their day to day work. It is easy to use and learn. Let’s dive in. The Chi-Squared test comes in three flavors. Goodness of Fit Test Test of Homogeneity Test of Independence 7.1 Goodness of Fit Test As the name implies, this test checks whether a given set of data conforms to (fit) a prespecified distribution. Consider the following example: Example: The following data is from a survey of Hamilton students conducted in 2016. The counts are survey respondents in each class year. class_frq &lt;- c(&quot;Freshman&quot; = 20, &quot;Sophomore&quot; = 32, &quot;Junior&quot; = 16, &quot;Senior&quot; = 27) class_frq ## Freshman Sophomore Junior Senior ## 20 32 16 27 pie(class_frq) Class Year Count Percent Freshman 20 21.1% Sophomore 32 33.7% Junior 16 16.8% Senior 27 28.4% Now, you might wonder, hmmmmm…, did we actually over sample two class years (seniors and sophomores) and under-sampled the other two classes or could this be due to chance variation? We can try to answer this by using the one sample proportions test on each group. That is we can conduct four hypothesis tests as follows. Test 1 Test 2 Test 3 Test 4 \\(H_0: p_{freshman} = 0.25\\) \\(H_0: p_{sophomore} = 0.25\\) \\(H_0: p_{junior} = 0.25\\) \\(H_0: p_{senior} = 0.25\\) \\(H_1: p_{freshman} \\neq 0.25\\) \\(H_1: p_{sophomore} \\neq 0.25\\) \\(H_1: p_{junior} \\neq 0.25\\) \\(H_1: p_{senior} \\neq 0.25\\) A question: Can you think why the above approach is not very good? I’ll let you ponder on this question. For the time being, let’s agree to not to use the above approach. So, what else can be done? This where the Chi squared goodness of fit test comes in to play. What we wanted to test is whether the student body in our college is evenly distributed between the 4 class years. The Chi squared test approach this issue in a slightly different way. \\(H_0:\\) Proportion of freshmen = 1/4 Proportion of sophomores = 1/4 Proportion of juniors = 1/4 Proportion of seniors = 1/4 \\(H_1:\\) The students are not evenly distributed between class years. So, IF the null hypothesis were true, out of the 95 students we’d expect about 95/4 (on average) falls into each class year give or take (with some random variation). For example, in our sample, we’d EXPECT about 23 (95/4) students to have come from each class year. We can then compare these expected counts with the actual OBSERVED counts. Consider the following table: Class Year Observed Count Expected Count Difference Freshman 20 23.75 -3.75 Sophomore 32 23.75 8.25 Junior 16 23.75 -7.75 Senior 27 23.75 3.25 The Chi-squared test aggregate these differences in to a single number. We call it the Chi-squared test statistic. Here’s how it aggregates: \\[ Chi-squared\\ Test\\ Statistic:\\ X^2 = \\sum_{sum\\ over\\ groups} \\frac{(Observed - Expected)^2}{Expected} \\] Now you may have couple of questions: Question 1: Why do we square the difference between observed and expected counts? If you simply add the differences the positive differences and negative differences will cancel one another. As a result we’ll end up with a zero as the final answer. That is not helpful. We need a way to aggregate the differences in a meaningful way. By squaring and adding them up we can overcome the cancellation issue. Also, it acts as a weight function. That is, smaller differences contribute less and larger differences contribute more to the final outcome. So you can think of the test statistic as a weighted average of squared deviations. Question 2: Why do we divide by the expected count? Case 1 Case 2 Observed = 10 Observed = 100 Expected = 5 Expected = 95 In both cases, the difference is 5. But we see that 5 is a substantial deviation for Case 1 than it is for Case 2. A better approach is to look at the relative difference. That is to calculate the difference between the observed and expected counts relative to what we’d expect, hence we divide by the expected count. There is another answer to the above questions using the Poisson distribution. But we haven’t studied the Poisson distribution in our intro courses so I will not get into that. Now let’s see how to do this in R. Command to run the test my_chisq_test &lt;- chisq.test(x = class_frq, p = c(1/4,1/4,1/4,1/4)) my_chisq_test ## ## Chi-squared test for given probabilities ## ## data: class_frq ## X-squared = 6.4316, df = 3, p-value = 0.0924 As you can see, you’ll get the Chi-squared test statistic and the corresponding p-value. How did they come up with the p-value? To answer this question, we need to go all the way back to 1900. Karl Pearson was the man who invented this method in a paper in 1900. He proved that, IF the null hypothesis were true, the sampling distribution of the above test statistic follows a Chi-squared distribution. So what is this Chi-squared distribution? The Chi-squared distribution is used to model positive values. The smallest value you can have is zero. It is a skewed distribution (mostly). The shape (and skewness) is governed by a parameter called the ‘degree of freedom’ parameter’ ( denoted by \\(k\\) ). It is a positive integer. For smaller \\(k\\) you see a lot of skewness and for large \\(k\\) the distribution becomes more symmetric. The mean of the distribution is equal to \\(k\\) and the variance is \\(2k\\). Here are two examples: Now let’s look back at our problem. We found that the corresponding Chi-squared value for the class year data is 6.43. Is this value a likely one or an unlikely one? To answer that question, we need to compare this value against a Chi-squared distribution. But, which distribution are we going to use? We need to know the \\(k\\) (degrees of freedom) parameter corresponds to our test statistic. In his paper in 1900 Karl Pearson figured out that the correct value of \\(k\\) is related to the number of groups that we are comparing. He proved that \\(k = ( \\# \\ of \\ groups \\ - 1 )\\). Therefore, in our case we need to work with a Chi-squared distribution with \\(k=3\\). The following plot show the correct distribution with the vertical line indicating the observed value of the Chi-squared test statistics (6.43) for the class year data. Do you think that this value is a likely value under the null hypothesis or is it unlikely? The observed value of 6.43 is somewhat in the unlikely region. We can calculate exactly the probability of observing 6.43 (or more) and that is nothing but the p-value. The p-value = 0.09. It is kind of ‘small’. Perhaps not that small to be considered as unlikely. This is where we need to dig deeper into the data. The following commands will help us put things in perspective before we jump in to make a decision about the p-value. Display the observed counts my_chisq_test$observed ## Freshman Sophomore Junior Senior ## 20 32 16 27 Display the expected counts my_chisq_test$expected ## Freshman Sophomore Junior Senior ## 23.75 23.75 23.75 23.75 Display the residuals (standardized, i.e. z-score) ## Freshman Sophomore Junior Senior ## -0.7694838 1.6928643 -1.5902664 0.6668859 We can make use of the residuals to help us reveal more about the data and the corresponding p-value. First we need to define what these residuals are. \\[ residuals \\ = \\ \\frac{Observed - Expected}{\\sqrt{Expected}} \\] You can think of them as Z-scores. For example, for sophomores, the observed count is 1.69 standard deviations above the expected count. For juniors, the observed count is 1.59 standard deviations below the expected count. Any residual value larger than 2 (roughly) is considered ‘unusual’. In this case non of the residuals are above 2, but they give us an important clue about the nature of the data. Consider the following dialog: You Can you explain why the residual value is somewhat high (1.69) for sophomores? Researcher: Given the context, I think they may have over-sampled the sophomores which explains the large positive residual. You: I still don’t get it. Why would they over-sample? Researcher: Well, this course is mainly taken by students who have had some prior Statistics background (College Intro Stats or AP stats). So typically we tend to have more sophomores and juniors in the class. Since the survey was done by the students in the class, they tend to interview members of their own class year hence the over-representation. You: OK, but what about the juniors? Why do they have an under-representation? The residual for juniors is negative (-1.59) which indicates that there is an under-representation. But your argument above suggests that they must over-sample the juniors too because the class is generally taken by more sophomores and juniors. I’m confused. Researcher: That is a good point. But in this case, we have a different explanation. You see, in our college, a considerable number of juniors tend to study abroad. As a result we always have an under-representation of the juniors on-campus in each year. Therefore, any on-campus survey would have a low representation of juniors. You: I see, that makes sense. But now I have a different question. Didn’t we assume (in the null hypothesis) that the class years are evenly distributed? According to what you just said, shouldn’t we be testing a different hypothesis which matches the reality of on-campus class distribution? Researcher: You are absolutely right. We should use the correct expected percentages in our null hypothesis. Let’s redo the analysis together. The correct expected on-campus class percentages are slightly different from 25%. The following table shows the enrollment numbers at the beginning of that academic year (source: Hamilton College Registrar’s office). Class Year Percentage Freshman 25.9% Sophomore 27.8% Junior 19.6% Senior 26.7% Perform the test again and examine the residuals my_chisq_test2 &lt;- chisq.test(x = class_frq, p = c(.259, .278, .196, .267)) my_chisq_test2 ## ## Chi-squared test for given probabilities ## ## data: class_frq ## X-squared = 2.5191, df = 3, p-value = 0.4718 my_chisq_test2$residuals ## Freshman Sophomore Junior Senior ## -0.9283633 1.0877463 -0.6071716 0.3246387 As you can see, the Chi-squared value is much smaller (2.52 compared to 6.43 earlier) hence the p-value is much larger ( p- val = 0.43 vs. p-val = 0.09 earlier). The following plot shows the observed Ch-squared value is quite likely under the null hypothesis. So what is our conclusion? We can say “there is not enough evidence to say that the observed counts differ significantly from we’d reasonably expect.”. In other words, the students have done a good job in getting a representative sample from the college. The apparent disparities in the observed counts is simply a result of chance variation. Let’s look at another example. Example: The main issue we are investigating here is racial disparity is jury selection in Alameda county in northern California. In 2010, the American Civil Liberties Union (ACLU) found large disparities in the racial makeup of the selected jurors in Alameda county. The data is from a jury pool of 1395 individuals. Our task is to see whether these disparities can be attributed to chance. In other words, we ask the familiar question: “Could this be due to chance?” First, let’s look at the data. The following table shows the counts of jurors from each racial group. Race Observed Count White 780 Black 117 Hispanic 114 Asian 384 We would like to compare these counts or (percentages) to the racial distribution of eligible jurors of that county. The following table summarizes the racial makeup of the eligible population of Alameda county in 2010. Race Eligible Percent White 54% Black 18% Hispanic 12% Asian 16% Following what we did with class year example we can use the Chi-squared test to see whether there is an unusual disparity in the observed and the expected counts. Perform the test and examine the residuals juror_counts &lt;- c(&#39;White&#39; = 780, &#39;Black&#39; = 117, &#39;Hispanic&#39; = 114, &#39;Asian&#39; = 384) jury_chisq_test &lt;- chisq.test(x = juror_counts, p = c(.54, .18, .12, .16)) jury_chisq_test ## ## Chi-squared test for given probabilities ## ## data: juror_counts ## X-squared = 205.44, df = 3, p-value &lt; 2.2e-16 jury_chisq_test$observed ## White Black Hispanic Asian ## 780 117 114 384 jury_chisq_test$expected ## White Black Hispanic Asian ## 753.3 251.1 167.4 223.2 jury_chisq_test$residuals ## White Black Hispanic Asian ## 0.9728083 -8.4626313 -4.1272762 10.7631390 The Chi-squared value is 205 which means the p-value is essentially zero. Hence we can safely say that observed counts are highly unlikely under the assumption of the racial makeup (of the eligible jurors) in Alameda county . Does this mean that there is racial discrimination in selecting jurors? We need to be very careful here. This test only allow us to safely rule out random chance as a plausible explanation. However, this allow us to push the local authorities for an alternative explanation for the disparities we see in the data. In addition, we should ALWAYS examine the residuals. They provide a richer picture of the data, something that we can’t get with the p-value. This is a very important point to keep in mind. In ANY analysis, the p-value is not the end, it is in fact the beginning of a long journey (analysis). The residuals from the above test reveal an interesting pattern. All minorities have residuals that are quite large (larger than 2 in magnitude). In particular, Blacks and Hispanics are underrepresented (8 and 4 SD’s below the expected counts respectively) while the Asians were over-represented by a huge margin (10 SD’s above the expected count!). These are vital pieces of information when we try to answer the racial discrimination question. Summary The Chi-squared goodness fit test helps us to test whether a given set of counts conforms to (fit) a prespecified distribution. The test statistic \\(X^2\\) has a Chi-squared distribution with \\((k-1)\\) degrees of freedom under the null hypothesis where \\(k\\) is the number of groups in the data. Large values of the test statistic provides evidence against the null hypothesis. ALWAYS look at the residuals of the test to get a better understanding of the data. Don’t just look at p-value. Remember, p-values are like mosquitoes! :) 7.2 Test of Homogeneity This is second variation of the Chi-squared test. The main purpose of this test is to test whether samples taken from several populations or entities have a similar distribution. In other words, this is sort of like doing the goodness-of-fit test ( what we just studied above) across multiple populations. Let’s look at an example. Example: Suppose we have five samples from 5 major hospitals in NYC. We wanted to see whether COVID-19 virus complication rates are similar in these five hospitals. Hospital 1 Hospital 2 Hospital 3 Hospital 4 Hospital 5 Respiratory distress 1129 661 789 350 2402 Cardiovascular shock 197 103 135 60 405 Arrhythmia 105 76 83 45 231 Kidney failure 73 21 67 23 174 # Create a table with 4 rows and 5 columns covid &lt;- matrix(0, nrow = 4, ncol = 5) # Name the rows and columns of the table rownames(covid) &lt;- c(&#39;Respiratory distress&#39;, &#39;Cardiovascular shock&#39;, &#39;Arrhythmia&#39;, &#39;Kidney failure&#39;) colnames(covid) &lt;- c(&#39;Hospital 1&#39;,&#39;Hospital 2&#39;,&#39;Hospital 3&#39;,&#39;Hospital 4&#39;,&#39;Hospital 5&#39;) # Fill in each column with the corresponding data covid[ , 1] &lt;- c(1129, 197, 105, 73) covid[ , 2] &lt;- c(661, 103, 76, 21) covid[ , 3] &lt;- c(789, 135, 83, 67) covid[ , 4] &lt;- c(350, 60, 45, 23) covid[ , 5] &lt;- c(2402, 405, 231, 174) # Simple bar chart barplot(covid, col = c(&#39;red&#39;,&#39;green&#39;,&#39;blue&#39;, &#39;yellow&#39;), legend.text = TRUE, args.legend = list(bty = &#39;n&#39;, x =&#39;right&#39;, ncol = 1), xlim = c(0,10)) # Note 1: In the above command legend.text&#39; = TRUE means include the legend. # args.legend = list(...stuff...) gives additional parameters for the legend. # bty = &#39;n&#39; means no border aroind the legend # x = &#39;right&#39; means position of the lenged # ncol = 1 means the number of columns in the legend. # Note 2: This is not the most efficient way to plot barcharts. # Using ggplot() is far better. But, I chose this approach because # having the data as a matrix, rather than a dataframe helps us do the # Chi-squared test quickly, which is the main goal of this chapter. The above plot is somewhat misleading. As you can see, the y-axis represents the counts. So, if the sample sizes are unequal, which is the case in our study, it is difficult to compare the distributions across the hospitals. Therefore we need to calculate the percentages of these disease categories so we can have a meaningful comparison of the hospitals. Consider the following plot. You can now easily compare the 5 hospitals. The distributions seem quite similar to the naked eye with some unequal parts. So you might ask, “could these apparent dissimilarities be a result of chance?”. This is where statistics is needed to discern whether the disease distributions are actually similar across the 5 hospitals. # Percentage bar chart -- margin = 2 means that we compute column percentages. covid_proportions &lt;- prop.table(covid, margin = 2) barplot(covid_proportions, col = c(&#39;red&#39;,&#39;green&#39;,&#39;blue&#39;, &#39;yellow&#39;), legend.text = TRUE, args.legend = list(bty = &#39;n&#39;, x =&#39;right&#39;, ncol = 1), xlim = c(0,10)) First we need to state our hypothesis. Our null hypothesis is that the distributions are identical across the 5 hospitals. We can denote this as follows: \\(H_0:\\) \\(P_{Respir|1} = P_{Respir|2} = \\ \\ \\ldots \\ldots \\ldots = P_{Respir|5} = P_{CommonR}\\) \\(P_{Cardio|1} = P_{Cardio|2} = \\ \\ \\ldots \\ldots \\ldots = P_{Cardio|5} = P_{CommonC}\\) \\(P_{Arrhyth|1} = P_{Arrhyth|2} = \\ \\ \\ldots \\ldots \\ldots = P_{Arrhyth|5} = P_{CommonA}\\) \\(P_{Kidney|1} = P_{Kidney|2} = \\ \\ \\ldots \\ldots \\ldots = P_{Kidney|5} = P_{CommonK}\\) \\(H_1:\\) The distributions are not similar across the 5 hospitals Now we need to to use the null hypothesis to find the expected counts for each disease. The null hypothesis states that there is no difference between the hospitals when it comes to the proportion of patients in each complication category. Therefore, we can pool the observed counts to estimate a common proportion for each category as follows: Total Percent Respiratory distress 5331 74.78% Cardiovascular shock 900 12.62% Arrhythmia 540 7.58% Kidney failure 358 5.02% Now we can ask, what fraction of cases from each hospital would we expect to see in each disease category. To complete this calculation we also need the total sample sizes of each hospital. The table below shows how many patients we sampled from each hospital. Hospital 1 Hospital 2 Hospital 3 Hospital 4 Hospital 5 Total cases sampled 1504 861 1074 478 3212 Now we have all the ingredients to calculate the expected counts in each category and for each hospital. For example, the sample size of hospital 1 was 1504. So we’d expect about 74.78% of these cases to be in the category of ‘Respiratory distress’. That is, Expected count in respiratory distress in Hospital 1 = 1504 * 0.7478 = 1124.69 Likewise we can calculate the expected counts for each category in hospital 1. Then repeat the same process for hospital 2 and so on. You might now wonder: : “Do we have to do this by hand?”. Of course not. We will use R to do all the heavy lifting. Before that I’d like to point out an important fact on calculating the expected counts. Let’s consider the above calculation with hospital 1 data. Note that the percentage of “Respiratory distress” was calculated by dividing the total cases in that category (across all hospitals) by the grand total which was 7129. So we can rewrite the calculation as follows: Expected count in respiratory distress in Hospital 1 \\[ = 1504 * 0.7478 \\] \\[ = 1504 * \\frac{5331}{7129}\\] If you look at this above expression carefully you will see that the expected count only depends on 3 things from the original data table. The column total (1504, the total cases in that hospital) The row total (5331, the total cases in respiratory distress across all hospitals) The grand total. The expected count for any hospital and disease category can be written as: \\[ Expected \\ Count \\ = \\ \\frac{Row\\ Total\\ *\\ Column\\ Total}{Grand\\ Total}\\] Perform the test mycovid_test &lt;- chisq.test(covid) mycovid_test ## ## Pearson&#39;s Chi-squared test ## ## data: covid ## X-squared = 22.449, df = 12, p-value = 0.03278 As you can see from the plot below the observed value of the Chi-squared test statistics (22.449) is in the unlikely region. How unlikely? About 3%. How do we interpret this p-value? The next thing that we should ALWAYS do is to examine the residuals. mycovid_test$residuals Hospital 1 Hospital 2 Hospital 3 Hospital 4 Hospital 5 Respiratory distress 0.129 0.676 -0.498 -0.394 0.002 Cardiovascular shock 0.517 -0.546 -0.050 -0.044 -0.025 Arrhythmia -0.836 1.335 0.183 1.461 -0.789 Kidney failure -0.291 -3.382 1.779 -0.205 1.000 Recall that these residuals are in the form of z-scores. That is, they tell you how far off the observed counts from the expected in terms of standard deviation units. Therefore, any residual that is larger than 2 is deemed unusual. When you examine the table above, you’ll notice something very interesting. None of the residuals seem “large” except the last value in Hospital 2. The residual value is -3.38 which tells us that, in Hospital 2, the number of observed kidney failures is 3.38 SD’s BELOW the expected value (under the assumptions that all hospitals have the same proportion in each disease category). This is a very important finding. There can be two plausible explanations for this unusual occurrence: Maybe the patients who get admitted to Hospital 2 already have kidney related issues or maybe more susceptible to kidney failures. Maybe Hospital 2 was doing something “right” (or different from others) when it comes to handling COVID-19 patients which led to lower than expected kidney failures. Possibility 1 seems a bit unlikely because these data come from random samples across NYC, so it is unlikely that ALL the kidney patients tend to cluster in one geographical area. Nonetheless, the doctors and health officials can (and should) check this by going through patient records. If we can verify that they DO NOT have any history of kidney related issues and don’t seem to have a special susceptibility to kidney failures then we can then move on to possibility 2. If Hospital 2 is doing something “right” it can share this knowledge with other hospitals so that they can try it on their patients and hopefully it MIGHT lead to similar (favorable) outcomes. We need to stress here that this is something we have do with utmost care, because these findings don’t necessarily mean that whatever Hospital 2 is doing is the CAUSE of the low kidney failure counts that we observed. It is only a CORRELATION. As we always say in this course, domain knowledge is key in any real life situation. We, the statisticians, can only report our findings in an easily understandable way. The rest is up to the experts in the relevant field. Let’s hope that they do the right thing! let’s look at another example to solidify our understanding. Example: Suppose you are interested in public opinion on gun laws in the state of New York. You also are interested whether these opinions tend to be similar (or different) across political party affiliations. You decided to use a telephone survey (typical means of doing public opinion surveys) to gather your data. Before collecting the data you got a list of registered Democrats, Republicans and Independents in the state of NY. You can make a freedom of information act request here : https://www.elections.ny.gov/FoilRequests.html to obtain this information. Now that you have a list of registered Democrats, Republicans and Independents, you can randomly pick say 200 from each group and call them to find out their opinions on gun laws. Suppose you conducted the survey successfully and obtained the following information. Democrat Republican Independent Need stricter gun laws 90 56 39 Need to loosen gun laws 17 103 23 Keep as is 93 41 138 200 200 200 # Create a table with 3 rows and 3 columns gun_opinion &lt;- matrix(0, nrow = 3, ncol = 3) # Name the rows and columns of the table rownames(gun_opinion) &lt;- c(&#39;Strict&#39;, &#39;Loose&#39;, &#39;As is&#39;) colnames(gun_opinion) &lt;- c(&#39;Democrat&#39;,&#39;Republican&#39;,&#39;Independent&#39;) # Fill in each column with the corresponding data gun_opinion[ , 1] &lt;- c(90, 17, 93) gun_opinion[ , 2] &lt;- c(56, 103, 41) gun_opinion[ , 3] &lt;- c(39, 23, 138) # Proportions bar chart gun_opinion_proportions &lt;- prop.table(gun_opinion, margin = 2) barplot(gun_opinion_proportions, col = c(&#39;red&#39;,&#39;green&#39;,&#39;blue&#39;), legend.text = TRUE, args.legend = list(bty = &#39;n&#39;, x =&#39;right&#39;, ncol = 1), xlim = c(0,5)) As you can see the three distributions are no where near close to be similar. I don’t think we need to perform any statistical analysis on this because the differences across the parties are so large that random chance can not be a plausible explanation. But, in order to examine the residuals (something we should ALWAYS do) we need to run the test. gun_opinion_test &lt;- chisq.test(gun_opinion) gun_opinion_test ## ## Pearson&#39;s Chi-squared test ## ## data: gun_opinion ## X-squared = 170.58, df = 4, p-value &lt; 2.2e-16 gun_opinion_test$residuals ## Democrat Republican Independent ## Strict 3.608049 -0.7216098 -2.886439 ## Loose -4.441802 8.0145555 -3.572754 ## As is 0.245049 -5.2160433 4.970994 Does the residuals tell us anything interesting? One thing we immediately see is that ALL residuals except two values are larger than 2. We all know that Republicans tend to favor loose gun laws and Democrats favor tighter gun laws. We can clearly see that in the residuals. But the magnitudes are very different. Republicans are 8 standard deviations above the expected count for loose gun laws. It is a huge deviation compared to the Democrats in supporting stricter gun laws (3.6 SD’s from the expected). I admit that the above example is not that interesting. However, I chose this example to compare and contrast an important point about data collection and how it effects our analysis. Let’s dive in. 7.3 Test of Independence Example: Look back at the previous example on gun laws. Notice how the data was collected in that example. It had two steps: Obtain a list of ALL registered voters. Then randomly sample 200 from each group (Dem, Rep, Ind). Our primary goal was to see whether the groups are similar (homogeneous) in terms of their opinions on gun laws. We used the test of homogeneity to answer that question. The hypothesis is about the conditional probabilities of opinion given that the person is from a certain political party. That is: \\[ P_{Opinion\\ |\\ Dem} = P_{Opinion\\ |\\ Repub} = P_{Opinion\\ |\\ Ind}\\] The conditional aspect comes from the way we collected the data. In our data collection, we measured only one random variable, the opinion. What do I mean by that? When we collected the data, we would call a Democrat, for example, and ask him or her to provide their opinion on gun laws. So we already knew that the person we are calling is a Democrat. So it is not a random variable. But the opinion of that person is unknown to us at the time of the call. So the opinion is the ONLY random (unknown) quantity that we measured. Now let’s consider a different approach to data collection. Rather than getting a list of ALL registered voters and sample from that list, what if we simply call 600 people via random digit dialing (the most typical method in opinion polls). At the time of the call we have no idea about TWO things: The party affiliation of the person Their opinion on gun laws. So we measure TWO random variables and we are interested in the interplay between these two variables. That is, we want to know the dependence or independence of these two variables. Hence the name, test of independence. Let us now see how to perform this test. Now that you’ve seen how a chi-squared test is done, let’s proceed with the relevant steps. Step 1: Define the null and the alternative hypotheses for the test. \\(H_0:\\) The two variables (Party affiliation and Opinion) are independent: of each other. \\(H_1:\\) The two variables are dependent. Step 2: Calculate the expected counts under the null hypothesis. First, let’s look at the data table below. Our total sample size is 600 but the column totals are different from the previous example. We no longer have exactly 200 members selected from each group because when you randomly call 600 people you have no control over the totals in each group. Remember, we are measuring TWO random variables, Party and Opinion. Democrat Republican Independent Need stricter gun laws 88 41 37 166 Need to loosen gun laws 18 97 31 146 Keep as is 91 56 141 288 197 194 209 600 Suppose we need to compute the expected count for row 1, column 1 of the table below. In this case, it is Democrats who support stricter gun laws. Now we ask, out of the 600 people how many do we expect to be in this category IF the null hypothesis were true? To find this we need to know (or estimate) the probability of a person being a Democrat AND support stricter gun laws if the null hypothesis were true. That is, we need \\(P[\\ Democrat\\ AND\\ Strict\\ ]\\) assuming that the two variables are independent. Recall from your intro stats the following result: \\[P[A \\ and \\ B] = P[A].P[B]\\] if and only if the two events \\(A\\) and \\(B\\) are independent. Using this, we can say, under the null hypothesis of independence \\[ P[\\ Democrat\\ AND\\ Strict\\ ] = P[\\ Democrat\\ ] .P[\\ Strict\\ ] \\] Then we can estimate the above probability as follows: \\[ P[\\ Democrat\\ ] = 197/600 \\] \\[ P[\\ Strict\\ ] = 166/600 \\] Therefore, \\[ P[\\ Democrat\\ AND\\ Strict\\ ] = \\frac{197}{600}.\\frac{166}{600} \\] No we can find, out of a 600 people how many would we expect to be in this category as follows: \\[ Expected \\ Count \\ = 600 * P[\\ Democrat\\ AND\\ Strict\\ ] = 600 . ( \\frac{197}{600}.\\frac{166}{600}) \\] If you examine above formula carefully you’ll notice that the expected count for any group and any opinion category can be written as: \\[ Expected \\ Count \\ = \\ \\frac{Row\\ Total\\ *\\ Column\\ Total}{Grand\\ Total}\\] This is identical to the formula we had in the test of homogeneity. But we arrived at this via a different path (different hypothesis). Step 3: Calculate the Chi-squared test statistic (\\(X^2\\)) and the associated p-value and interpret your findings in context. # Fill in the table with the data gun_opinion[ , 1] &lt;- c(88, 18, 91) gun_opinion[ , 2] &lt;- c(41, 97, 56) gun_opinion[ , 3] &lt;- c(37, 31, 141) # Perform the test gun_opinion_test &lt;- chisq.test(gun_opinion) gun_opinion_test ## ## Pearson&#39;s Chi-squared test ## ## data: gun_opinion ## X-squared = 141.47, df = 4, p-value &lt; 2.2e-16 Let’s look at another example before we end this chapter. Example: There is a growing concern that Black drivers get searched more (compared to Whites) in regular traffic stops. The ‘crime’ ironically called “Driving while Black”. The following dataset is a random sample of traffic stops in Cincinnati in 2002. Black White Other Searched? Yes 813 293 19 Searched? No 787 594 27 Question: Is there any association between the driver’s race and being searched? Let’s use the Chi-squared test of independence to answer this question. First, let us state the null and the alternative hypotheses. \\(H_0:\\) The two variables, Race and Searched in a traffic top, are independent: of each other. \\(H_1:\\) The two variables are dependent. search_data &lt;- matrix(0, nrow = 2, ncol=3) rownames(search_data) &lt;- c(&quot;Searched? Yes&quot;, &quot;Searched? No&quot;) colnames(search_data) &lt;- c(&#39;Black&#39;, &#39;White&#39;, &#39;Other&#39;) search_data[ , 1] &lt;- c(813, 787) search_data[ , 2] &lt;- c(293, 594) search_data[ , 3] &lt;- c(19, 27) search_test &lt;- chisq.test(search_data) search_test ## ## Pearson&#39;s Chi-squared test ## ## data: search_data ## X-squared = 73.253, df = 2, p-value &lt; 2.2e-16 The p-value is essentially zero which indicates that IF race and subject to a search is INDEPENDENT then it is highly unlikely that we observed the counts in the above table. Therefore, we can conclude that there an ASSOCIATION between these two variables. However, we need to be super careful not to jump into a more stronger assertion like “There is a discrimination at traffic stops against black individuals” straight away. There might be many reasons behind this. To get a better understanding of the situation, let’s not forget to examine the residuals. It might shed more light into our analysis. search_test$residual ## Black White Other ## Searched? Yes 3.840584 -5.086104 -0.3164431 ## Searched? No -3.432987 4.546322 0.2828593 The value that jumps out is the -5.08 in the White group. This suggests that IF race and search are independent, then the 293 searches in the White group is 5 standard deviations BELOW the expected count. This is huge. Why is it that the White group experienced a significantly less number of searches? On the other hand, the 813 searches in the Black group is about 4 standard deviations ABOVE the expected count. Why is the Black group experienced a significantly higher number of searches? These are the kinds of analyses you’ll need when you want confront officials. Without these types of ammunition your case is weak. The p-value is not sufficient to convince other (reasonable) people, you need more. As I mentioned before, the p-value is not the end, it is the beginning of a long journey (analysis). Summary We’ve learned the three flavors of the Chi-Squared test. Goodness of Fit Test (Check the distribution of a variable is in accordance to a per-specified distribution) Test of Homogeneity ( Check whether the distribution of a variable is similar across several populations or groups) Test of Independence ( Check whether TWO variables are correlated) The process of carrying out the test can be summarized as follows: Step 1: Define the null and the alternative hypotheses for the test. Step 2: Calculate the expected counts under the null hypothesis. Step 3: Calculate the Chi-squared test statistic (\\(X^2\\)) and the associated p-value. Interpret your findings in context. Step 4: Examine the residuals to find out any other interesting facts about the data that may be hidden. "],["the-logistic-regression-model.html", "Chapter 8 The Logistic Regression Model 8.1 The Failure of the Linear Model 8.2 Construction of a New Model 8.3 Fitting a Logistic Model", " Chapter 8 The Logistic Regression Model Very simply, this model is an extension of the classical regression model. The primary goal of this model is to predict the probability of a binary outcome. Let’s begin with a very simple story. I owe this to Andy Fields, a professor in England. Here’s what he said in one of his books (Discovering Statistics using R). “Suppose we want to find out which variables are closely related in predicting whether a person is male or female. We might measure laziness, pig-headedness, alcohol consumption, and number of burps that a person does in a day. Using these characteristic variables we can build a logistic regression model and use it to predict whether a new person (who is not part of our original data) is likely to be a male or a female. For example, if we pick a random person and discovered that this person scored high in laziness, pig-headedness, alcohol consumption, and number of burps, then the logistic model might predict, based on this information, that this person is likely to be male. Admittedly, it is unlikely that a researcher would ever be interested in the relationship between flatulence and gender (it is probably too well established by experience to warrant research).” I hope you had a good laugh while reading the above quote. Now let us look at a real example (a more practical one). 8.1 The Failure of the Linear Model Suppose you are interested in finding whether a tumor is cancerous or benign and which variables are influential in determining the malignancy of the tumor. The following table shows data from a collection of breast tumors with some characteristics of the tumor and whether they are malignant or benign (indicated by the variable status). status radius texture perimeter area smoothness compactness concavity concave.points symmetry fractal.dimension M 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 0.2419 0.07871 M 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 M 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 0.2069 0.05999 M 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 0.2597 0.09744 M 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 0.1809 0.05883 M 12.45 15.70 82.57 477.1 0.12780 0.17000 0.1578 0.08089 0.2087 0.07613 This dataset is from the UC Irvine machine learning repository. Please see the following webpage for more details of this data: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic) Suppose we want to know the connection between the radius of the cell and malignancy. The relationship of these two variables is shown in the following plot. As you can see, cells that are ‘big’ (large radius) tend to be malignant. If we need to utilize this fact to estimate the probability of malignancy of a cell we can create a simple table by partitioning the x-axis into bins and calculate the proportion of malignant cells in each bin. This will give us a very simple estimate (see table below) of the probability of malignancy of a cell and how it is related to the radius. Radius (6,7] (7,8] (8,9] (9,10] (10,11] (11,12] (12,13] (13,14] (14,15] (15,16] (16,17] (17,18] (18,19] (19,20] Proportion 0 0 0 0 0.026 0.058 0.080 0.244 0.322 0.813 0.783 0.962 1 1 Plot with Estimates of Probability (local averages) The most important thing to pay attention in the above plot is the range of the local proportions. By definition, all proportions are bounded between [0,1]. Therefore, any model that tries to capture the trend of these proportions has to abide by this restriction. Suppose we try to use the classical simple linear model to capture this trend. You will immediately see why this model is not suitable in this context as shown in the following plot. The linear model may produce non-sencical predicted values. That is, probabilities bigger than 1 or less than 0. For example, in this instance, for radius values bigger than 20 or so you&#39;ll end up with predicted probabilities larger than 1! 8.2 Construction of a New Model How to overcome this issue? One reasonable approach might be to smooth out the local proportions (black dots) as shown in the following plot. This approach seem to be quite good. But it has some shortcomings. On rare occasions we might end up with probabilities bigger than 1 (or less than 0) It does not allow us to adequately quantify the effect of radius on the probability of malignancy. Let’s unpack issue number 2 above. Consider the following dialog: You: I’m not sure what you mean in issue number 2 above. Me:: What I mean by this is we are unable to answer questions like, “how much the probability might increase (or decrease) for different values of the \\(X\\) variable?”. You: Yes we can. We can look at the smooth line and eyeball (roughly estimate) the difference in probabilities. Me: Sure, that’s true. But it is a very rough estimate. If you are dealing with a life threatening situation like malignancy of a tumor, don’t you think you’d rather have a more precise method than just eyeballing? You: I guess that’s reasonable. Me: We need something like the simple linear regression model \\(\\hat Y = a + bX\\) where the “effect” of \\(X\\) is adequately captured by a model parameter (like the slope of the line). The coefficient of \\(X\\), \\(b\\), is a summary value of the “effect” of \\(X\\) on the response \\(Y\\). We need a similar model that can adequately capture (and summarize) the relationship between \\(X\\) and \\(Y\\). You: So how do we build such a model? Me: That’s what we are going to learn in the rest of this chapter. Let’s dive in. What we need is a model which does not exceed 1 for large \\(X\\) values and which does not go below 0 for small \\(X\\) values. There are several mathematical functions that obey these restrictions. One such function is the Logistic function. The mathematical equation for this function and its plot is given below: \\[ f(x) = \\frac{e^x}{1 + e^x} \\] This function seems to be good solution for our problem. Let’s try to fit this curve to our data. As you can see, the curve does not fit the data at all. The main issue is that it is centered at the wrong place. Q: How to shift this curve to the right? Answer: We need some type of an &quot;intercept&quot; parameter similar to what we have in a linear model. Let’s shift this curve to the right by 15 units. The equation of this new curve and its corresponding plot is given below: \\[ f(x) = \\frac{e^{-15 + x}}{1 + e^{-15 + x}} \\] Note: You might wonder why we have -15 instead of 15. This is not an error. In fact, this is one of your HW problems. Hint: Think about a much simpler function like \\(f(x) = x^2\\). How would you shift this, say 15 units to the right (x-direction)? As the above plot shows this new curve seem to fit the data really well. Perhaps, we can adjust the the steepness of the curve to fit the data points (local proportions) in the mid section. We can do that by introducing a parameter (like a “slope”) to the curve as shown in the following equation and the plot: \\[ f(x) = \\frac{e^{-15 + 1.05x}}{1 + e^{-15 +1.05x}} \\] Now you might be wondering whether we have to use this trial an error approach in fitting this curve to the data. The answer is: No. We have a systematic way to find the location and the shape of the logistic function. That is the focus in the next section. 8.3 Fitting a Logistic Model The logistic model that we fit to the data has the following general form: \\[ f(x) = \\frac{e^{a + bx}}{1 + e^{a + bx}} \\] The coefficients \\(a\\) and \\(b\\) are called model parameters. As you saw earlier these two coefficients are like two dials which controls two attributes of the curve. Coefficient Role \\(a\\) Controls the location of the curve. It centers the curve where the data is. \\(b\\) Controls the steepness (shape) of the curve. Similar to what we did earlier (using trial and error), our goal is to find the “curve of best fit”. The technique we use is called the maximum likelihood estimation. Let’s try to get our feet wet on this new concept. Maximum Likelihood Estimation This is one of the celebrated and time tested techniques in statistics. It is an intuitive method with many desirable properties and it is easy to implement using any statistical software. Let’s look at some toy examples to understand the fundamentals behind this technique. Example 1: Cookie jar, number of chocolate chip cookies, \\(\\theta\\), is either 6 or 2. That is, the set of possible values for \\(\\theta\\) is \\(\\{6,2\\}\\). We call this set the parameter space of \\(\\theta\\). \\(\\theta = \\{6,2\\}\\) Now suppose you picked a one cookie at random and found out that it is a chocolate chip cookie. Based on this sample (`evidence’) what is the most likely value of \\(\\theta\\)? If you guessed 6 you’re correct. Our intuition tells us that it is more likely that we get a chocolate-chip cookie if \\(\\theta\\) were 6 than if \\(\\theta\\) were 2. We can make this argument more precise by calculating the likelihood of getting a chocolate-chip cookie for the two values of \\(\\theta\\). \\[P[ X = CC\\ cookie\\ |\\ \\theta = 6] = 6/10 \\] \\[P[ X = CC\\ cookie \\ |\\ \\theta = 2] = 2/10 \\] Based on this sample of size 1, our choice for \\(\\theta\\) is 6. Let’s make it more interesting. What if we draw two cookies without replacement and found out that both of them are NOT chocolate chip cookies. Now what is your guess for the value of \\(\\theta\\) ? Again, we can calculate the probabilities for this outcome under the two scenarios \\(\\theta = 6\\) and \\(\\theta = 2\\). \\[P[ X_1 = NotCC, X_2 = NotCC \\ |\\ \\theta = 6] = (4/10) . (3/9) = 0.13 \\] \\[P[ X_1 = NotCC, X_2 = NotCC \\ |\\ \\theta = 2] = (8/10) . (7/9) = 0.80 \\] Now based on this sample of size 2, our choice for \\(\\theta\\) is 2. I hope now you’ll start to see a pattern. We pick a value for the parameter \\(\\theta\\) based on the likelihood of the observed (realized) outcomes in our sample. That is, we choose the value (of \\(\\theta\\)) to make our observed outcome most likely. This is the principle behind maximum likelihood estimation. Let’s look at another example. Example 2: Suppose I give you a coin from my country Sri Lanka and told you that this is an unbiased coin. Knowing who I am, you might suspect whether I’m being truthful about this coin. Here is a picture of my coin. Figure 8.1: Two sides of a Sri Lankan Coin. We label the side with the emblem (left picture) as heads. Suppose you are interested in the unbiased nature of my coin. If we denote \\(\\theta\\) as the probability of heads, then \\(\\theta\\) could be any value between [0,1]. If I’m being truthful, you’d say \\(\\theta = 0.5\\) (unbiased coin). You decided to test my claim of unbiasedness by flipping it 10 times. Suppose after 10 flips you got the following sequence of heads and tails. \\[HHHTHHTTHH\\] Based on this evidence, what is the most likely value of \\(\\theta\\)? You might guess \\(\\theta\\) to be 0.7. If you did, you effectively found the maximum likelihood estimate of \\(\\theta\\). Recall that we choose the value of \\(\\theta\\) that maximizes the likelihood of the observed (realized) sample. You might wonder, could there be any other value (other than 0.7) which yields a higher likelihood for this observed outcome \\(HHHTHHTTHH\\). Let’s check with some other values of \\(\\theta\\). \\(\\theta\\) Probability of \\(HHHTHHTTHH\\) outcome 0.1 0.00000 0.2 0.00000 0.3 0.00007 0.4 0.00035 0.5 0.00097 0.6 0.00179 0.7 0.00222 0.8 0.00167 0.9 0.00047 How did I calculate the above probabilities? Answer If we denote the probability of heads as \\(\\theta\\) and denote \\(y_i=1\\) if the \\(i^{th}\\) flip is a head and \\(y_i=0\\) if the \\(i^{th}\\) flip is a tails, then we can write the likelihood of the observed outcome as follows: \\[P[HHHTHHTTHH] = \\prod_{i=1}^{10} \\theta^{y_i}(1-\\theta)^{1-y_i} = \\theta^7(1-\\theta)^3\\] Note 1: The symbol \\(\\prod_{i=1}^{10}\\) is the product operator. It is a notation that we use to simplify things. Note 2: The above probability is a function of \\(\\theta\\). We call this function, the likelihhod function, \\(L(\\theta)\\). Then, by plugging in values \\(0.1,0.2, \\ldots, 0.9\\) to the above equation yields the corresponding probabilities. In fact, we can be even more precise. If you plot the above function \\(L(\\theta\\)) you’ll get the following plot: As shown in the above plot, the maximum of the likelihood function \\(L(\\theta)\\) occurs at \\(\\theta = 0.7\\). Example 3: In this example, let’s extend the intuition of the maximum likelihood estimation to the logistic model. Consider the following simple dataset, a subset of breast cancer dataset that you saw in earlier examples. How to use the Maximum Likelihood method to find the correct logistic curve? Recall my Sri Lankan coin flipping example. The breast cancer data can be thought of as a special coin flipping experiment where the probability of malignancy, \\(p\\), is dependent on the radius of the cell. That is, \\(p\\) is a function of radius. In particular, we claim that this probability can be modeled by a logistic curve. That is, \\[ p(radius) = \\frac{e^{a + b(radius)}}{1 + e^{a + b(radius)}}\\] If you look at the above plot you can see that there are 6 malignant cases and 14 benign cases. So we can write down the likelihood of observing the outcome \\([\\ MMMMMM\\ BBBBBBBBBBBBBB \\ ]\\). If we denote \\(y_i=1\\) for malignant cases and \\(y_i=0\\) for benign cases, we can write the required probability as follows: \\[P[\\ MMMMMM\\ BBBBBBBBBBBBBB \\ ] = \\prod_{i=1}^{20} p(radius_i)^{y_i}(1-p(radius_i))^{1-y_i} \\] Note 1: The symbol \\(\\prod_{i=1}^{20}\\) is the product operator. It is a notation that we use to simplify things. Note 2: The outcome of this expression is a mess. But, if you look at it carefully it is a simple function of 3 things: The model parameters \\(a\\) and \\(b\\), and the radius values (data) of the 20 patients. In order to get a better sense, take a look at the plot below. It is the plot of the likelihood for this dataset. Rotate the plot using your mouse to see where the likelihood is maximized. term estimate std.error statistic p.value (Intercept) -16.823 7.313 -2.300 0.021 radius 1.221 0.555 2.202 0.028 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
