

## Explorations with Real Data

In this section, we will look at 3 examples with real data.  Keep in mind, these examples are hard to find.  Because, it is very unlikely that we have ALL the members (data) available from a population.  If we have all data points from a population, there is no need for statistical inference!  


These examples are chosen to highlight one of the main misconceptions of CLT, namely, the '$n>30$ rule' (the title of this chapter).  In each example, when you look at the sampling distribution of the mean, pay close attention to the sample size.   Ask yourself, at what sample size does the sampling distribution start to look more like a normal model.


### NBA Player Salaries {-}


```{example}
NBA player salaries in 2016 season (Kolby Bryant's last season).  As with any other salary distrubution, these values are skewed with few players earning a LOT.
```

\

The following plots show the population distribution of NBA player salaries in 2016.


```{r echo=FALSE, message=FALSE, warning=FALSE}

NBA2016 <- read.csv(file = 'https://raw.githubusercontent.com/gitcnk/Data/master/NBA2016.csv')

variable_of_interest <- NBA2016$Salary/1000000

ggplot() + 
  aes( x = variable_of_interest) +
  geom_histogram() +
  labs(title = 'NBA Player Salaries',
       subtitle = 'Season: 2016',
       x = 'Salary in Million $')
```

\

The following plots show the sampling distributions are made from samples from this **population**.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height = 8, fig.width=12}

all_in_one <- read.csv(file = 'https://raw.githubusercontent.com/gitcnk/Data/master/NBA2016_all_in_one.csv')


g_20 <- all_in_one %>% filter( group == 'n=20')

plot_mean20 <- ggplot(data = g_20) +
                aes(x = mean) +
                geom_histogram() +
                labs(
                     subtitle = 'Sample size n = 20',
                     x = ' ')


g_30 <- all_in_one %>% filter( group == 'n=30')

plot_mean30 <- ggplot(data = g_30) +
                aes(x = mean) +
                geom_histogram() +
                labs(
                     subtitle = 'Sample size n = 30',
                     x = ' ')


g_50 <- all_in_one %>% filter( group == 'n=50')

plot_mean50 <- ggplot(data = g_50) +
  aes(x = mean) +
  geom_histogram(bins = 35) +
  labs(
    subtitle = 'Sample size n = 50',
    x = ' ')

g_100 <- all_in_one %>% filter( group == 'n=100')

plot_mean100 <- ggplot(data = g_100) +
                aes(x = mean) +
                geom_histogram(bins = 35) +
                labs(
                     subtitle = 'Sample size n = 100',
                     x = ' ')


grid.arrange( plot_mean20, 
              plot_mean30, 
              plot_mean50, 
              plot_mean100, 
              nrow = 2, ncol = 2)


```
\

**Observations**

The first thing you will notice that at $n=30$, the skewness is quite visible.  At $n=50$, it is still slightly skewed.  So may be we ougth to modify the "rule" as $n>50$?


### Undergraduates in US Colleges and Universities {-}

The second example is from ALL US colleges and universities.  Note that there are about 5000 colleges and universities in the US.  However, there are many missing datapoints since it is hard to find all data from all institutions.  As a result the population size is 1269.  In this example, we are intrested in the undergraduate population in US colleges and universities.  Here is the population ditribution.

```{r echo=FALSE, message=FALSE, warning=FALSE}

colleges <- read.csv(file = 'https://raw.githubusercontent.com/gitcnk/Data/master/colleges2016.csv')

variable_of_interest <- colleges$undergrads

ggplot() + 
  aes( x = variable_of_interest) +
  geom_histogram() +
  labs(title = 'Undergraduates in US colleges',
       subtitle = 'Year 2016',
       x = 'Salary')
```

\

The following plots show the sampling distributions are made from random samples from the above **population**.

```{r echo=FALSE, message=FALSE, warning=FALSE,  fig.height = 8, fig.width=12}

all_in_one <- read.csv(file = 'https://raw.githubusercontent.com/gitcnk/Data/master/Colleges_all_in_one.csv')


g_20 <- all_in_one %>% filter( group == 'n=20')

plot_mean20 <- ggplot(data = g_20) +
                aes(x = mean) +
                geom_histogram() +
                labs(
                     subtitle = 'Sample size n = 20',
                     x = ' ')


g_30 <- all_in_one %>% filter( group == 'n=30')

plot_mean30 <- ggplot(data = g_30) +
                aes(x = mean) +
                geom_histogram() +
                labs(
                     subtitle = 'Sample size n = 30',
                     x = ' ')


g_50 <- all_in_one %>% filter( group == 'n=50')

plot_mean50 <- ggplot(data = g_50) +
                  aes(x = mean) +
                  geom_histogram(bins = 35) +
                  labs(
                    subtitle = 'Sample size n = 50',
                    x = ' ')

g_100 <- all_in_one %>% filter( group == 'n=100')

plot_mean100 <- ggplot(data = g_100) +
                    aes(x = mean) +
                    geom_histogram(bins = 35) +
                    labs(
                         subtitle = 'Sample size n = 100',
                         x = ' ')


grid.arrange( plot_mean20, 
              plot_mean30, 
              plot_mean50, 
              plot_mean100, 
              nrow = 2, ncol = 2)


```

\

Even this example, at $n=30$ the skewness is quite visible.  At $n=50$ the skewness is still there.   Even at $n=100$, if you look at carefully you'll see that there is a slight skewness.  So may be the rule should be $n>100$?


### Departure Delays in LaGauadia Airport {-}

Now, let's look at the final example.  This data is about fligth delays.  You can find a lot of information and dowload data for past years from this website: https://www.transtats.bts.gov/ONTIME/Departures.aspx

The dataset we are looking at consists of ALL flights that departed from LaGuadia airport in 2017 from the three main carriers (American Airlines, Delta, and United).  That is, all depatures from Jan 1 2017 to Dec 31 2017.  The variable of interest is departure delay time.  First, let's take a look at the population distribution of departure delays.



```{r echo=FALSE, message=FALSE, warning=FALSE}

flights <- read.csv(file = 'https://raw.githubusercontent.com/gitcnk/Data/master/FlightDelays2018.csv')

variable_of_interest <- flights$Departure.delay..Minutes.

ggplot(flights) + 
  aes( x = variable_of_interest) +
  geom_histogram() +
  labs(title = 'Departure Delay Times',
       subtitle = 'LaGuadia Airport 2017',
       x = 'Departure Delay (in minutes)')
```

Now let's look at the corresponding sampling distribution for the sample mean.

\

```{r echo=FALSE, message=FALSE, warning=FALSE,  fig.height = 8, fig.width=12}

all_in_one <- read.csv(file = 'https://raw.githubusercontent.com/gitcnk/Data/master/FlightDelays_all_in_one.csv')


g_20 <- all_in_one %>% filter( group == 'n=20')

plot_mean20 <- ggplot(data = g_20) +
                  aes(x = mean) +
                  geom_histogram() +
                  labs(
                       subtitle = 'Sample size n = 20',
                       x = ' ')


g_30 <- all_in_one %>% filter( group == 'n=30')

plot_mean30 <- ggplot(data = g_30) +
                    aes(x = mean) +
                    geom_histogram() +
                    labs(
                         subtitle = 'Sample size n = 30',
                         x = ' ')


g_50 <- all_in_one %>% filter( group == 'n=50')

plot_mean50 <- ggplot(data = g_50) +
                  aes(x = mean) +
                  geom_histogram(bins = 35) +
                  labs(
                    subtitle = 'Sample size n = 50',
                    x = ' ')

g_100 <- all_in_one %>% filter( group == 'n=100')

plot_mean100 <- ggplot(data = g_100) +
                    aes(x = mean) +
                    geom_histogram(bins = 35) +
                    labs(
                         subtitle = 'Sample size n = 100',
                         x = ' ')


grid.arrange( plot_mean20, 
              plot_mean30, 
              plot_mean50, 
              plot_mean100, 
              nrow = 2, ncol = 2)


```
\

You can clearly see that none of the above plots look normally distributed.  Let us increase the sample size even further and see at what point it starts to look normal.


```{r echo=FALSE, message=FALSE, warning=FALSE,  fig.height = 8, fig.width=12}

all_in_one <- read.csv(file = 'https://raw.githubusercontent.com/gitcnk/Data/master/FlightDelays_all_in_one2.csv')


g_300 <- all_in_one %>% filter( group == 'n=300')

plot_mean300 <- ggplot(data = g_300) +
                    aes(x = mean) +
                    geom_histogram() +
                    labs(
                         subtitle = 'Sample size n = 300',
                         x = ' ')


g_600 <- all_in_one %>% filter( group == 'n=600')

plot_mean600 <- ggplot(data = g_600) +
                    aes(x = mean) +
                    geom_histogram() +
                    labs(
                         subtitle = 'Sample size n = 600',
                         x = ' ')


g_1000 <- all_in_one %>% filter( group == 'n=1000')

plot_mean1000 <- ggplot(data = g_1000) +
                      aes(x = mean) +
                      geom_histogram(bins = 35) +
                      labs(
                        subtitle = 'Sample size n = 1000',
                        x = ' ')

g_2000 <- all_in_one %>% filter( group == 'n=2000')

plot_mean2000 <- ggplot(data = g_2000) +
                    aes(x = mean) +
                    geom_histogram(bins = 35) +
                    labs(
                         subtitle = 'Sample size n = 2000',
                         x = ' ')


grid.arrange( plot_mean300, 
              plot_mean600, 
              plot_mean1000, 
              plot_mean2000, 
              nrow = 2, ncol = 2)


```

The above plots underscore the main point of this chapter.  The common belief that if $n>30$ the CLT is applicable for sample means.  This is a very miguided notion.  The above plots demonstrate that for this dataset the sample size $n$ should be around 1000(!) to see a symmetric, bell shaped sampling distribution.

By now, hopefully, you are convinced that there is a problem in the common understanding of the Central Limit Theorem.  The main point in this chapter is to be aware of this issue and be very cautious in using the CLT.

\
Run the following command in RStudio to open the data and do this exlopration yourself.  It will open an app that allows you to change the sample size and construct sampling distributions on your own.

\

```{r eval=FALSE, message=FALSE, warning=FALSE, include=TRUE}
library(shiny)
runGitHub(repo = "gitcnk/Apps/", subdir='CLT_NBA')
```




```{r eval=FALSE, message=FALSE, warning=FALSE, include=TRUE}
library(shiny)
runGitHub(repo = "gitcnk/Apps/", subdir='CLT_Colleges')
```
\

## Some Questions to Ponder


```{}
1. Why do people advocate for the "n > 30 rule" if it is problematic?
```

**Answer:**  Hard to say.  My guess is that this is similar to our beliefs and pratices of recycling plastics.  That is, we were told to toss our plastics into the recycling bin and we normally believe that those plastics will get recycled somehow. By the reality is MUCH more complicated than that.  Here is a wonderful documentary: https://www.youtube.com/watch?v=-dk3NOEgX7o

Similarly, the Central Limit Theorem should be used with caution.  There are many factors that we need to look into before we jump in and use this theorem.  The issue mainly lies in understanding the population and its distribution.  Since we don't have access to the population (if we did we'll be all be at the beach!), we have to rely on a random sample to make a judgement about the variability, skewness and outlier in the population.  This is a very challenging task.  For example, recall the flight delay example.  If we haven't had the entire dataset with us, would we have guessed that the population may look  exremely skewed?  Would we have guessed that there might be flights that are delayed 20 hours!  Consider an example about cancer medication and the survial time.  Unless we have an indepth knowledge about the cancer and the drug, it would be very difficult to even have a vague idea about the popluation distribution.  In this case we need to rely on domain experts to tell us more about variance, skewness and extreme values.

In realilty we only have a single sample from a given population and we need to make a judgement call on whether CLT is an appropriate technique to use with this sample.  That's why it is better to have a healthy level of skepticim about our data before we proceed.  As good statisticians, it is our duty to inform our clients about the limitations of the data and the infrences that we draw from them.



```{}
2. What are some of the signs that CLT may be questionble?
```

**Answer:**  If you look back the simulated data examples and real data examples you'll see that, for **skewed data**, it takes much larger sample size (in some cases in the 100's) to use the CLT.  Also, in the final example with departure delyas, the popualtion was not only highly skewed but also contained extremely large values in the tail.  This is definitely a red flag.  So if you suspect that the population is skewed and/or with extreme values you need to be super careful with the CLT.


```{}
3. How can we know whether the population is skewed or not when we don't have access 
to the entire population?
```


**Answer:** This one is tricky.  Yes, we'll never have access to the entire population.  All we have is a single random sample.  If our sampling process was good in capturing the variance in the population then it will provide important clues about the skeness and precence of extreme values in the popualtion.  For example, here are two random sample of sizes $n=30$  and $n=100$ respectively from the flight delay example.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height = 4, fig.width=12}
set.seed(1234)
mysample1 <- sample(variable_of_interest, size = 30)
mysample2 <- sample(variable_of_interest, size = 100)

ex_30 <- ggplot() +
            aes(x = mysample1) +
            geom_histogram() +
            labs(title = 'Departure Delay Times',
                 subtitle = 'Sample size n = 30',
                 x = 'Departure Delay (in minutes)')

ex_100 <- ggplot() +
            aes(x = mysample2) +
            geom_histogram() +
            labs(title = 'Departure Delay Times',
                 subtitle = 'Sample size n = 100',
                 x = 'Departure Delay (in minutes)')

grid.arrange(ex_30, ex_100, nrow = 1, ncol = 2)  
```



As you can see from the above two plots, the bigger sample was able able to capture more of the variance in the poulation but still failed to include some of the really large delaya times even with $n=100$.  But, this is where the statisticians need to step in.  We can raise a red flag agaist anyone who is tempted to use the CLT with a sample like this for hypothesis testing or confidence intervals.  We can educate them to see the danger of using CLT with a sample like this.  

```{}
4. If we see extreme values (like in the above two samples) should we remove them 
and proceed to use the CLT?
```

**Answer:** The safe answer is 'no'.  Removing data points has to be done with extreme care.  There are some instances where removing extreme values may be legitimate.

- If the data values are recoreded incorrectly (errors in the data).  For example, someone might have keyed in 10,000 (incorrect) instead of 1000 (correct).  If we know for sure that this is the case, first we should try to find the correct value, if not remove it.

- If the purpose of the analysis dictates the removal of extreme values.  Can you think of a situation where it might be essential to remove extreme values?  This will be a part of your HW for this chapter.


